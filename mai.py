import streamlit as st
import pandas as pd
import re
import plotly.graph_objects as go


import streamlit as st

st.set_page_config(page_title="App con M√∫ltiples P√°ginas", layout="wide")

# Ruta o URL del logo
logo_path = "Red.jpg"  # Si es local, usa el nombre del archivo
#st.image("ucol_logo.PNG", width=150)  # Ajusta el ancho seg√∫n necesites
#st.image(logo_path, use_container_width=True)
    
# Crear un contenedor con tres columnas y colocar la imagen en el centro
col1, col2, col3 = st.columns([1, 2, 1])  # La columna central es m√°s ancha

with col2:  # Colocar la imagen en la columna central
    st.image(logo_path, width=400)  # Ajusta el tama√±o seg√∫n necesites



# Crear men√∫ de navegaci√≥n
pagina = st.selectbox("Selecciona una p√°gina", ["Inicio", "An√°lisis por base", "An√°lisis de temas por √°rea", "An√°lisis por autor", "Equipo de trabajo"])

# Mostrar contenido seg√∫n la p√°gina seleccionada
if pagina == "Inicio":
    st.title("Producci√≥n y redes de colaboraci√≥n cient√≠fica de la Universidad de Colima")

    st.markdown("""
<div style='text-align: justify'>
Esta aplicaci√≥n est√° ideada para la visualizaci√≥n y an√°lisis de la producci√≥n cient√≠fica de los investigadores de la Universidad de Colima. En su versi√≥n m√°s reciente, se utilizan los datos disponibles en la base de <a href="https://www.scopus.com" target="_blank"><strong>Scopus</strong></a>, la cual indexa art√≠culos, libros y patentes derivados del trabajo cient√≠fico en todo el mundo. Mediante el uso de diversas t√©cnicas de machine learning, se examina la 
<strong>productividad de los investigadores de la Universidad de Colima, las tendencias de publicaci√≥n (tanto tem√°ticas como editoriales), las √°reas de conocimiento predominantes y las redes de colaboraci√≥n nacional e internacional</strong>.
</div>
""", unsafe_allow_html=True)

    st.subheader("Objetivo")

    
    st.markdown("""
    <div style='text-align: justify'>
El objetivo es proporcionar una herramienta interactiva que permita a investigadores, tomadores de decisiones y p√∫blico interesado <strong>explorar visualmente la evoluci√≥n y caracter√≠sticas de la actividad cient√≠fica institucional</strong>.

Este an√°lisis puede contribuir a:

- Fortalecer estrategias de vinculaci√≥n y colaboraci√≥n.

- Identificar l√≠neas de investigaci√≥n consolidadas y emergentes.

- Apoyar procesos de evaluaci√≥n y planeaci√≥n acad√©mica.

Los datos han sido obtenidos mediante consultas espec√≠ficas a Scopus y procesados con herramientas de an√°lisis de datos y visualizaci√≥n.
</div>
""", unsafe_allow_html=True)



    st.markdown(
            """
    <div style='text-align: justify'>

### Prop√≥sito

Algunas de las car√°ctes√≠sticas y propositos de esta aplicaci√≥n son:

- **Interfaz adaptada a necesidades locales:** La visualizaci√≥n se limita exlusivamente a la producci√≥n cient√≠fica en la que han participado investigadores de la Universidad de Colima.

- **Visualizaciones personalizadas e interactivas:** se muestran gr√°ficos, tablas din√°micas, filtros por autor, a√±o o tema, seg√∫n el inter√©s del usuario.

- **Difusi√≥n institucional:** Ideado para comunicadores y responsables de investigaci√≥n que buscan comprender y comunicar los resultados de manera clara.
    </div>
    """,
    unsafe_allow_html=True
    )



    st.markdown(
    """
    <div style='text-align: justify'>

    ### T√©cnicas de an√°lisis y visualizaci√≥n empleadas

    Para facilitar la comprensi√≥n y exploraci√≥n de los datos, esta aplicaci√≥n incorpora diversas t√©cnicas de an√°lisis bibliom√©trico y visualizaci√≥n de datos mediante machine learning:

    - **√Årboles de decisi√≥n:** facilitan la clasificaci√≥n de datos bibliogr√°ficos y la detecci√≥n de factores asociados a mayores niveles de productividad o impacto.

    - **Clustering jer√°rquico:** agrupa autores, instituciones o t√©rminos clave en funci√≥n de su similitud, lo que ayuda a descubrir patrones de colaboraci√≥n o l√≠neas tem√°ticas emergentes.

    - **Diagramas de caja (boxplots):** ofrecen una visi√≥n clara de la distribuci√≥n de m√©tricas como el n√∫mero de citas o documentos por autor, permitiendo identificar *outliers* y analizar la variabilidad.

    - **Grafos de correlaci√≥n:** visualizan relaciones entre variables (como coautor√≠as, coocurrencia de palabras clave o correlaciones entre m√©tricas), destacando estructuras y v√≠nculos relevantes.

    - **s de palabras:** permiten identificar r√°pidamente los t√©rminos m√°s frecuentes en t√≠tulos, res√∫menes y palabras clave, revelando temas recurrentes en la producci√≥n cient√≠fica.


    </div>
    """,
    unsafe_allow_html=True
    )

    st.markdown(
        """
        ### Nota aclaratoria
        
    <div style='text-align: justify'>
    Los datos fueron obtenidos de manera autorizada mediante acceso institucional a Scopus. <strong>Esta aplicaci√≥n no est√° afiliada ni es respaldada por Elsevier</strong>. Los resultados son con fines educativos e informativos.


    </div>
    """,
    unsafe_allow_html=True
    )



###############################################################################################################################
elif pagina == "An√°lisis por base":
    
#    @st.cache_data

    import streamlit as st
    import pandas as pd

    # üìå T√≠tulo de la aplicaci√≥n
    st.title("An√°lisis tem√°tico de autores de publicaciones cient√≠ficas")

    st.markdown(
    """
    <div style='text-align: justify'>
    En esta secci√≥n se analizan algunos aspectos claves de los autores de la <strong>Universidad de Colima</strong> de publicaciones indizadas en la base de datos de <strong>Scopus</strong>. Algunos de estos aspectos son: la identificaci√≥n de los autores mas prol√≠ficos, la evoluci√≥n temporal de sus publicaciones, su distribuci√≥n de autores de acuerdo a su productividad y un clasificador en el que el usuario puede comparar su productividad con la de los autores de la base. 
    </div>
    """,
    unsafe_allow_html=True
    )

    
    st.markdown("""
    **Para poder visualizar el an√°lisis de publicaciones, por favor cargue la base de datos de publicaciones de Scopus.**
    """)
    
    # üìÇ **Subir archivo CSV**
    uploaded_file = st.file_uploader("Sube un archivo CSV", type=["csv"])

    if uploaded_file is not None:
        # Cargar el archivo en un DataFrame

        
        df = pd.read_csv(uploaded_file, encoding='utf-8')

        # ‚úÖ Mostrar mensaje de √©xito
        st.success("‚úÖ Archivo cargado correctamente.")

        # üìä **Mostrar las primeras filas**
        #st.subheader("üìã Vista previa de los datos")
        #st.markdown("""
        #Estas son las **primeras cinco filas** del archivo con la lista de publicaciones en las que se han involucrado profesores de la Universidad de Colima. Cada fila corresponde a un art√≠culo diferente. En las secciones posteriores, esta base se separar√° para generar un registro de la productividad cient√≠fica individual de los profesores de la Universidad de Colima.
        #""")

        # Asegurar que el √≠ndice est√© limpio
        df = df.reset_index(drop=True)

        # Eliminar columnas que no quieres conservar
        columnas_a_eliminar = [
            'DOI', 'Link', 'Page start', 'Page end', 'Page count',
            'Funding Texts', 'ISSN', 'ISBN', 'CODEN', 'Open Access'
        ]
        df = df.drop(columns=columnas_a_eliminar, errors='ignore')

        
        #st.write(df.head())
        #with st.expander("**Datos del archivo**"):
        #    st.write(f"**N√∫mero de filas:** {df.shape[0]}")
        #    st.write(f"**N√∫mero de columnas:** {df.shape[1]}")
        #    st.write("**Lista de columnas:**")
        #    st.write(df.columns.tolist())  # Mostrar los nombres de las columnas como una lista

            
        # üìÇ **Descargar el archivo procesado**
        #csv_data = df.to_csv(index=False).encode('utf-8')
        #st.download_button("**Descargar CSV**", csv_data, "datos_procesados.csv", "text/csv")


        import re

        # üìå **Funci√≥n para procesar los datos**
        def process_author_data(df):
            df.columns = df.columns.str.strip().str.replace(" ", "_")  # Reemplazar espacios en nombres de columnas

            # Verificar que las columnas necesarias est√°n en el DataFrame
            if "Author_full_names" not in df.columns or "Author(s)_ID" not in df.columns:
                st.error("‚ùå No se encontraron las columnas 'Author full names' o 'Author(s) ID'.")
                return None

            # Crear diccionarios para asignar nombres a los IDs de autores
            author_id_map = {}
            author_name_map = {}

            for row in df.dropna(subset=["Author_full_names"]).itertuples(index=False):
                author_entries = str(getattr(row, "Author_full_names")).split(";")
                for entry in author_entries:
                    match = re.match(r"(.*) \((\d+)\)", entry.strip())  # Extraer nombre completo e ID
                    if match:
                        full_name, author_id = match.groups()
                        author_id_map[author_id] = full_name
                        author_name_map[author_id] = full_name.split(",")[0]  # Solo apellido y primera inicial

            # Expandir filas con m√∫ltiples IDs separados por ';'
            df = df.assign(**{"Author(s)_ID": df["Author(s)_ID"].astype(str).str.split(";")}).explode("Author(s)_ID")
            df["Author(s)_ID"] = df["Author(s)_ID"].str.strip()

            # Asignar nombres basados en los IDs de autor
            df["Author_full_names"] = df["Author(s)_ID"].map(author_id_map).fillna("Unknown Author")
            df["Authors"] = df["Author(s)_ID"].map(author_name_map).fillna("Unknown Author")

            return df

        df_processed = process_author_data(df)

        if df_processed is not None:
            #st.success("‚úÖ Datos procesados correctamente.")
            st.write(" ")
            # üìã **Vista previa**
            #st.subheader("üìã Vista previa de los datos procesados")
            #st.markdown("""
            #Como primer paso, se procesa la base de datos para crear un registro por autor de cada art√≠culo producido. De esta manera, cada fila corresponde a una ocasi√≥n en la que un investigador participa en un art√≠culo.
            #""")

            #st.write(df_processed.head())

            #with st.expander("Datos del archivo"):
            #    st.write(f"**N√∫mero de filas:** {df_processed.shape[0]}")
            #    st.write(f"**N√∫mero de columnas:** {df_processed.shape[1]}")
            #    st.write("**Lista de columnas:**")
            #    st.write(df_processed.columns.tolist())  # Mostrar los nombres de las columnas como una lista

            # üìÇ **Descargar el archivo procesado**
            #csv_data = df_processed.to_csv(index=False).encode("utf-8")
            #st.download_button("üì• Descargar datos procesados", csv_data, "processed_author_data.csv", "text/csv")

        from collections import Counter
        import streamlit as st
        import pandas as pd
        import re
        from collections import Counter

#st.title("üìä Procesamiento de Datos de Autores")

# üìÇ **Subir archivo CSV**
#uploaded_file = st.file_uploader("Sube un archivo CSV", type=["csv"])


        # üìå **Funciones para procesamiento y an√°lisis de datos**
        def process_author_data(df):
            df.columns = df.columns.str.strip().str.replace(" ", "_")
        
            if "Author_full_names" not in df.columns or "Author(s)_ID" not in df.columns:
                st.error("‚ùå No se encontraron las columnas 'Author full names' o 'Author(s) ID'.")
                return None

            author_id_map = {}
            author_name_map = {}

            for row in df.dropna(subset=["Author_full_names"]).itertuples(index=False):
                author_entries = str(getattr(row, "Author_full_names")).split(";")
                for entry in author_entries:
                    match = re.match(r"(.*) \((\d+)\)", entry.strip())
                    if match:
                        full_name, author_id = match.groups()
                        author_id_map[author_id] = full_name
                        author_name_map[author_id] = full_name.split(",")[0]

            df = df.assign(**{"Author(s)_ID": df["Author(s)_ID"].astype(str).str.split(";")}).explode("Author(s)_ID")
            df["Author(s)_ID"] = df["Author(s)_ID"].str.strip()

            df["Author_full_names"] = df["Author(s)_ID"].map(author_id_map).fillna("Unknown Author")
            df["Authors"] = df["Author(s)_ID"].map(author_name_map).fillna("Unknown Author")
        
            return df
    
        df_processed = process_author_data(df)

        if df_processed is not None:
            st.write(" ")
        
            # üìã **Vista previa**
            #st.subheader("üìã Vista previa de los datos procesados")
            #st.write(df_processed.head())

            # üìä **An√°lisis de Editoriales y Publicaciones**
            st.subheader("Autores con mayor producci√≥n")

            def count_unique_publishers(publishers):
                if isinstance(publishers, float) and pd.isna(publishers):
                    return 0
                return len(set(str(publishers).split(";")))

            def sorted_frequent_publishers(publishers):
                if isinstance(publishers, float) and pd.isna(publishers):
                    return ""
                publisher_list = str(publishers).split(";")
                counter = Counter(publisher_list)
                sorted_publishers = sorted(counter.items(), key=lambda x: x[1], reverse=True)
                return "; ".join(f"{pub} ({count})" for pub, count in sorted_publishers)

            def format_year_counts(years):
                if isinstance(years, float) and pd.isna(years):
                    return ""
                year_list = str(years).split(";")
                counter = Counter(year_list)
                sorted_years = sorted(counter.items(), key=lambda x: int(x[0]))
                return "; ".join(f"{year} ({count})" for year, count in sorted_years)

            df_grouped = df_processed.groupby("Author(s)_ID").agg({
                "Cited_by": "sum",
                "Title": "count",
                "Source_title": lambda x: len(x.unique()),
                "Funding_Details": lambda x: x.notna().sum(),
                "Year": lambda x: format_year_counts(";".join(map(str, x))),
                **{col: lambda x: "; ".join(map(str, x.unique())) for col in df_processed.columns if col not in ["Cited_by", "Title", "Source_title", "Funding_Details", "Year", "Author(s)_ID"]}
            }).reset_index()

            df_grouped = df_grouped.rename(columns={"Title": "Publications", "Source_title": "Journals", "Funding_Details": "Funded_publications"})
            df_grouped["Not_funded_publications"] = df_grouped["Publications"] - df_grouped["Funded_publications"]

            if "Publisher" in df_processed.columns:
                df_grouped["Publisher_Count"] = df_grouped["Publisher"].apply(count_unique_publishers)
                df_grouped["Most_frequent_publisher"] = df_grouped["Publisher"].apply(sorted_frequent_publishers)

            columns_to_drop = ["DOI", "Volume", "Issue", "Art._No.", "Page_start", "Page_end", "Page_count", "Link", "ISBN", "CODEN", "Funding_Texts", "ISSN", "Open_Access", "Publisher"]
            df_grouped = df_grouped.drop(columns=columns_to_drop, errors="ignore")
            #st.markdown("""
 
            #""")

#            st.markdown(
#            """
#            <div style='text-align: justify'>
#           Como primer paso, se separa la base original, generando una fila para cada participaci√≥n de un autor o autora de la Universidad en un art√≠culo indizado. Debido a que es posible que una persona aparezca con distintas versiones de su nombre, as√≠ como con distintos identificadores, se normalizaron los nombres (eliminando may√∫sculas y caracteres especiales), y se unificaron todos los trabajos de cada autor en el ID mas reciente. Adem√°s, se us√≥ el registro de direcciones de correo para depurar la base y conservar solo a aquellos autores que tengan una direcci√≥n que pueda asociarse con la Universidad de Colima (esto √∫ltimo implic√≥ una revisi√≥n por parte de la Direcci√≥n General de Investigaci√≥n Cient√≠fica para quitar del registro a colaboradores que no pertenezcan a la Universidad de Colima). A continuaci√≥n se muestran las <strong>primeras cinco filas</strong> de la base con los autores ya separados.
#            </div>
#            """,
#            unsafe_allow_html=True
#            )

            
            st.markdown(
            """
            <div style='text-align: justify'>
           Como primer paso, se separa la base original, generando una fila para cada participaci√≥n de un autor o autora en un art√≠culo indizado. Debido a que es posible que una persona aparezca con distintas versiones de su nombre, as√≠ como con distintos identificadores, se normalizaron los nombres (eliminando may√∫sculas y caracteres especiales), y se unificaron todos los trabajos de cada autor en el ID mas reciente. Adem√°s, se us√≥ el registro de direcciones de correo para depurar la base y conservar solo a aquellos autores que tengan una direcci√≥n que pueda asociarse con la Universidad de Colima (esto √∫ltimo implic√≥ una revisi√≥n por parte de la Direcci√≥n General de Investigaci√≥n Cient√≠fica para quitar del registro a colaboradores que no pertenezcan a la Universidad de Colima). Debido a las pol√≠ticas de reproducci√≥n de datos protegidos, no se muestra directamente la tabla de autores. Adem√°s, se reemplaza el identificador de scopus por un Folio. Si desea conocer el nombre de la persona ala que pertenece un folio, consulte el diccionario.
            </div>
            """,
            unsafe_allow_html=True
            )
            
            #st.markdown("""
            #    Despu√©s del procesamiento, se agrupa la informaci√≥n a nivel de autor ("Author(s)_ID") y se generan estad√≠sticas:
            #    - "Normalized_Author_Name": Nombre del autor, escrito en min√∫sculas y sin caracteres especiales.
            #    - "Cited_by": Suma de citas recibidas.
            #    - "Publications": Conteo de art√≠culos por autor.
            #    - "Journals": N√∫mero de fuentes √∫nicas en las que ha publicado.
            #    - "Funded_publications": Cantidad de art√≠culos con financiamiento.
            #    - "Not_funded_publications": Publicaciones sin financiamiento (Publications - Funded_publications).
            #    - "Year": A√±os en los que el autor public√≥ (los n√∫meros entre par√©ntesis representan el n√∫mero de publicaciones por a√±o).
            #    - "Authors_ID": ID de scopus del autor o autora.
            #    - "Publisher_Count" y "Most_frequent_publisher": Cantidad y ranking de editoriales (si la columna "Publisher" est√° en el DataFrame).
            #""")
            #st.dataframe(df_grouped)
            # Definir el orden prioritario
            priority_columns = [
                "Author(s)_ID",
                "Authors",
                "Author_full_names",
                "Publications",
                "Year",
                "Cited_by",
                "Funded_publications",
                "Not_funded_publications",
                "Correspondence_Address"
            ]

            # Obtener las dem√°s columnas sin alterar su orden original
            remaining_columns = [col for col in df_grouped.columns if col not in priority_columns]

            # Reordenar el DataFrame con las columnas prioritarias primero
            df_grouped = df_grouped[priority_columns + remaining_columns]

            # Mostrar las primeras filas para verificar
            #st.write(df_grouped.head())


            
            #with st.expander("Datos del archivo"):
            #    st.write(f"**N√∫mero de filas:** {df_grouped.shape[0]}")
            #    st.write(f"**N√∫mero de columnas:** {df_grouped.shape[1]}")
            #    st.write("**Lista de columnas:**")
            #    st.write(df_grouped.columns.tolist())  # Mostrar los nombres de las columnas como una lista



            
            #csv_data = df_grouped.to_csv(index=False).encode("utf-8")
            #st.download_button("üì• Descargar datos agrupados", csv_data, "unified_author_data.csv", "text/csv")

#####################################################

        # Filtrar filas donde la columna "Correspondence_Address" contenga variantes de "Universidad de Colima"
        keywords = ["Colima", "UCOL", "COLIMA", "UdeC", "ucol"]
        df_ucol = df_grouped[df_grouped["Correspondence_Address"].str.contains('|'.join(keywords), case=False, na=False)]

        #df_ucol
        # Guardar el resultado en un nuevo archivo CSV
        #df_ucol.to_csv("author_data_colima.csv", index=False)

        # Filtrar filas donde la columna "Correspondence_Address" contenga variantes de "Universidad de Colima"
        keywords = ["@ucol"]
        df_ucol_dir = df_ucol[df_ucol["Correspondence_Address"].str.contains('|'.join(keywords), case=False, na=False)]

        #df_ucol_dir
        # Guardar el resultado en un nuevo archivo CSV
        #df_ucol_dir.to_csv("author_data_colima_dir.csv", index=False)

        # Filtrar las filas que NO contienen las palabras clave en la columna 'Correspondence_Address'
        keywords = ["Colima", "UCOL", "COLIMA", "UdeC", "ucol"]
        df_not_ucol = df_grouped[~df_grouped["Correspondence_Address"].str.contains('|'.join(keywords), case=False, na=False)]

        # Guardar el resultado en un nuevo archivo CSV
        #df_not_ucol.to_csv("author_data_not_colima.csv", index=False)
        #df_not_ucol

#########################################################################
#########################################################################
        import unicodedata

        # Lista de autores a eliminar
        authors_to_remove = ["crossa,", "murillo zamora, efren", "guzman esquivel,", "martinez fierro"]

        # Funci√≥n mejorada para normalizar nombres y eliminar iniciales, espacios extra y puntos finales
        def normalize_name_v2(name):
            if pd.isna(name):
                return ""
            name = name.lower().strip()  # Convertir a min√∫sculas y quitar espacios extra
            name = unicodedata.normalize('NFKD', name).encode('ASCII', 'ignore').decode('utf-8')  # Eliminar acentos
            name = re.sub(r'[-_]', ' ', name)  # Reemplazar guiones y guiones bajos por espacios
            name = re.sub(r'\s+', ' ', name)  # Reemplazar m√∫ltiples espacios por un solo espacio
            name = re.sub(r'\b([A-Z])\b', '', name, flags=re.IGNORECASE)  # Eliminar iniciales de segundo nombre
            name = re.sub(r'\.$', '', name)  # Eliminar puntos al final del nombre
            name = name.strip()  # Quitar espacios extra resultantes
            return name

        # Aplicar la normalizaci√≥n mejorada a los nombres de autores
        df_ucol_dir["Normalized_Author_Name"] = df_ucol_dir["Author_full_names"].apply(normalize_name_v2)
        # Eliminar los autores no deseados antes de la agrupaci√≥n
        df_ucol_dir = df_ucol_dir[~df_ucol_dir["Normalized_Author_Name"].isin(authors_to_remove)]


        # Funci√≥n para sumar correctamente las publicaciones por a√±o
        def sum_year_counts(year_entries):
            year_count = Counter()
            for entry in year_entries.dropna():
                matches = re.findall(r'(\d{4})\s*\((\d+)\)', entry)
                for year, count in matches:
                    year_count[year] += int(count)
            sorted_years = sorted(year_count.items(), key=lambda x: int(x[0]))
            return "; ".join(f"{year} ({count})" for year, count in sorted_years)

        # Agrupar por el nombre normalizado para fusionar datos de autores con el mismo nombre
        df_ucol = df_ucol_dir.groupby("Normalized_Author_Name").agg({
            "Cited_by": "sum",
            "Publications": "sum",
            "Journals": "sum",
            "Funded_publications": "sum",
            "Not_funded_publications": "sum",
            "Year": lambda x: sum_year_counts(x),  # Sumar correctamente los valores entre par√©ntesis
            **{col: lambda x: "; ".join(map(str, x.unique())) for col in df_ucol_dir.columns if col not in [
                "Cited_by", "Publications", "Journals", "Funded_publications", "Not_funded_publications", "Year", "Normalized_Author_Name"]}
        }).reset_index()

        # Crear un diccionario para rastrear qu√© filas se fusionan
        merge_log = {}
        for name in df_ucol["Normalized_Author_Name"].unique():
            merged_ids = df_ucol_dir[df_ucol_dir["Normalized_Author_Name"] == name]["Author(s)_ID"].unique()
            if len(merged_ids) > 1:  # Solo registrar si hubo m√°s de una fusi√≥n
                merge_log[name] = list(merged_ids)

        # Convertir el log en un DataFrame para visualizar las fusiones
        df_merge_ucol_log = pd.DataFrame(list(merge_log.items()), columns=["Normalized_Author_Name", "Merged_Author_IDs"])

        # Crear columna Folio tipo UCOL-0001, UCOL-0002, ...
        df_ucol['Folio'] = ['UCOL-' + str(i).zfill(4) for i in range(1, len(df_ucol) + 1)]

        # Crear diccionario de correspondencia entre Folio y Author(s) ID
        diccionario_folios = dict(zip(df_ucol['Folio'], df_ucol['Author(s)_ID']))

        #with st.expander("**Datos del archivo**"):
        #    st.write(f"**N√∫mero de filas:** {df_ucol.shape[0]}")
        #    st.write(f"**N√∫mero de columnas:** {df_ucol.shape[1]}")
        #    st.write("**Lista de columnas:**")
        #    st.write(df_ucol.columns.tolist())  # Mostrar los nombres de las columnas como una lista
            
        #csv_data = df_ucol.to_csv(index=False).encode("utf-8")
        #st.download_button("**Descargar datos ucol**", csv_data, "unified_ucol_author_data.csv", "text/csv")

##########################################################################

        # Recargar librer√≠as
        import pandas as pd
        import plotly.express as px

        # Asegurar que la columna Year es de tipo string y separar los valores correctamente
        df_ucol["Year"] = df_ucol["Year"].astype(str)

        # Expandir la columna Year para contar las publicaciones por a√±o por autor
        df_expanded = df_ucol.assign(Year=df_ucol["Year"].str.split(";")).explode("Year")

        # Extraer solo el a√±o num√©rico y la cantidad de publicaciones en ese a√±o
        df_expanded[["Year", "Publications"]] = df_expanded["Year"].str.extract(r'(\d{4})\s*\((\d+)\)')

        # Convertir los valores a tipo num√©rico
        df_expanded["Year"] = pd.to_numeric(df_expanded["Year"], errors='coerce')
        df_expanded["Publications"] = pd.to_numeric(df_expanded["Publications"], errors='coerce')
        df_expanded = df_expanded.dropna(subset=["Year", "Publications"])  # Eliminar filas con valores no v√°lidos

        # Ordenar cronol√≥gicamente los a√±os
        df_expanded = df_expanded.sort_values(by=["Year", "Normalized_Author_Name"])

        # Filtrar el DataFrame eliminando a los autores no deseados
        authors_to_remove = ["crossa,", "murillo zamora, efren", "guzman esquivel,", "martinez fierro,"]
        df_expanded_filtered = df_expanded[~df_expanded["Normalized_Author_Name"].isin(authors_to_remove)]

        # Identificar los 30 autores con m√°s publicaciones totales despu√©s del filtrado
        top_authors_filtered = df_expanded_filtered.groupby("Normalized_Author_Name")["Publications"].sum().nlargest(30).index

        # Filtrar solo los datos de los 30 autores principales
        df_top30_filtered = df_expanded_filtered[df_expanded_filtered["Normalized_Author_Name"].isin(top_authors_filtered)].copy()

        # Obtener el primer y √∫ltimo a√±o en la lista
        year_min = df_top30_filtered["Year"].min()
        year_max = df_top30_filtered["Year"].max()

        # Crear una columna de publicaciones acumuladas
        df_top30_filtered["Cumulative_Publications"] = 0

        # Diccionario para rastrear la acumulaci√≥n de publicaciones por autor    
        author_cumulative_filtered = {author: 0 for author in top_authors_filtered}

        # Lista para almacenar los frames de la animaci√≥n
        frames_filtered = []

        # Iterar a√±o por a√±o y actualizar el n√∫mero acumulado de publicaciones
        for year in range(year_min, year_max + 1):
            # Obtener las publicaciones de los autores en el a√±o actual
            df_year = df_top30_filtered[df_top30_filtered["Year"] == year].copy()

            # Actualizar los valores acumulados para cada autor en el a√±o actual
            for author in top_authors_filtered:
                if author in df_year["Normalized_Author_Name"].values:
                    # Sumar publicaciones de este a√±o
                    publications_this_year = df_year[df_year["Normalized_Author_Name"] == author]["Publications"].sum()
                    author_cumulative_filtered[author] += publications_this_year

            # Crear un DataFrame con los valores actualizados
            df_snapshot = pd.DataFrame({
                "Normalized_Author_Name": list(author_cumulative_filtered.keys()),
                "Cumulative_Publications": list(author_cumulative_filtered.values()),
                "Year": year
            })

            # Filtrar los 30 autores con m√°s publicaciones acumuladas hasta el momento
            df_snapshot = df_snapshot.sort_values(by=["Cumulative_Publications", "Normalized_Author_Name"], ascending=[False, True]).head(30)

            # Agregar el snapshot a la lista de frames
            frames_filtered.append(df_snapshot)

        # Unir los datos en un solo DataFrame para la animaci√≥n
        df_final_filtered = pd.concat(frames_filtered)

        # Determinar el valor m√°ximo de publicaciones acumuladas en cada a√±o
        df_max_values_filtered = df_final_filtered.groupby("Year")["Cumulative_Publications"].max().reset_index()
        df_max_values_filtered["Cumulative_Publications"] = df_max_values_filtered["Cumulative_Publications"] * 1.1  # A√±adir margen del 10%

  #      # Agregar el Author(s)_ID al DataFrame antes de generar la gr√°fica
  #      df_final_filtered = df_final_filtered.merge(df_ucol[["Normalized_Author_Name", "Authors_ID"]], on="Normalized_Author_Name", how="left")

        # Obtener el √∫ltimo a√±o de la animaci√≥n
  #      last_year = df_final_filtered["Year"].max()

#        # Extraer el orden final de los autores basado en el √∫ltimo a√±o
  #      final_order = df_final_filtered[df_final_filtered["Year"] == last_year].sort_values(
#            by="Cumulative_Publications", ascending=False
#        )["Normalized_Author_Name"].tolist()

#        # Extraer el orden final de los autores basado en el √∫ltimo a√±o
#        final_order = df_final_filtered[df_final_filtered["Year"] == last_year].sort_values(
#            by="Cumulative_Publications", ascending=False
#        )["Normalized_Author_Name"].tolist()


        
        # Crear la gr√°fica de barras animada con acumulaci√≥n, orden final fijo y Author(s)_ID en hover
#        fig_filtered = px.bar(
#            df_final_filtered,
 #           x="Cumulative_Publications",
 #           y="Normalized_Author_Name",
 #           color="Normalized_Author_Name",
 #           animation_frame="Year",
 #           orientation="h",
 #           title="Evoluci√≥n de Publicaciones Acumuladas - Top 30 Autores",
 #           labels={"Cumulative_Publications": "N√∫mero Acumulado de Publicaciones", "Normalized_Author_Name": "Autores"},
            #hover_data={"Authors_ID": True},  # Agregar el ID del autor en el hover
 #           template="plotly_white"
 #       )

 #       # Aplicar el orden inverso en el eje Y para que los autores con m√°s publicaciones est√©n abajo
 #       fig_filtered.update_layout(
 #           xaxis=dict(range=[0, df_max_values_filtered["Cumulative_Publications"].max()]),
 #           height=1000,  # Aumentar la altura para evitar que los nombres se aplasten
 #           yaxis=dict(categoryorder="array", categoryarray=final_order[::-1])  # Invertir el orden de los autores
 #       )



        
        # Mostrar la gr√°fica interactiva con la correcci√≥n en hover
        #st.plotly_chart(fig_filtered)


###########################################################################################

        import re
        import unicodedata
        from collections import Counter
        import plotly.express as px
        import matplotlib.pyplot as plt
        import os
        import numpy as np

        #st.title("üìä Procesamiento y An√°lisis de Publicaciones Acumuladas")
        st.markdown(
    """
    <div style='text-align: justify'>
        A continuaci√≥n se muestra <strong>la evoluci√≥n temporal de los 30 autores mas productivos de la Universidad de Colima</strong>. La animaci√≥n puede reproducirse al presionar el bot√≥n de la parte superior izquierda del gr√°fico. Puede ajustar el a√±o a visualizar mediante el deslizador de la parte inferior.
    </div>
    """,
    unsafe_allow_html=True
    )
        
        # Lista de autores a eliminar
        authors_to_remove = ["crossa,", "murillo zamora, efren", "guzman esquivel,", "martinez fierro,"]
        df_final_filtered = df_final_filtered[~df_final_filtered["Normalized_Author_Name"].isin(authors_to_remove)]

        # Obtener los a√±os en orden
        years_sorted = sorted(df_final_filtered["Year"].unique())
        year_min = min(years_sorted)
        year_max = max(years_sorted)

        # Calcular publicaciones por a√±o para cada autor
        #df_final_filtered["Yearly_Publications"] = df_final_filtered.groupby(["Normalized_Author_Name", "Year"])["Cumulative_Publications"].diff().fillna(df_final_filtered["Cumulative_Publications"])
        df_final_filtered["Yearly_Publications"] = df_final_filtered.groupby(["Normalized_Author_Name", "Year"])["Cumulative_Publications"].diff().fillna(df_final_filtered["Cumulative_Publications"])

        # Crear la gr√°fica de barras animada con estratificaci√≥n por a√±o
        fig = px.bar(
            df_final_filtered,
            x="Yearly_Publications",
            y="Normalized_Author_Name",
            #y="Folio",
            #color="Normalized_Author_Name",
            animation_frame="Year",
            orientation="h",
            title="Evoluci√≥n de Publicaciones Acumuladas - Top 30 Autores",
            labels={"Yearly_Publications": "Publicaciones en el A√±o", "Normalized_Author_Name": "Autores"},
            template="plotly_white"
        )


        for frame in fig.frames:
            year = frame.name
            frame.data += (go.Scatter(
                x=[df_final_filtered["Yearly_Publications"].max() * 1.05], 
                y=[df_final_filtered["Normalized_Author_Name"].min()],
                mode="lines",
                line=dict(color="black", width=2),
                name=f"A√±o {year}"
            ),)


        # Ajustar layout para la animaci√≥n
        fig.update_layout(
            xaxis=dict(range=[0, df_final_filtered["Yearly_Publications"].max() * 1.1]),
            height=1000,
            yaxis=dict(categoryorder="total ascending"),
            updatemenus=[
                {
                    "buttons": [
                        {
                            "args": [None, {"frame": {"duration": 500, "redraw": True}, "fromcurrent": True}],
                            "label": "Play",
                            "method": "animate"
                        },
                        {
                            "args": [[None], {"frame": {"duration": 0, "redraw": True}, "mode": "immediate", "transition": {"duration": 0}}],
                            "label": "Pause",
                            "method": "animate"
                        }
                    ],
                    "direction": "left",
                    "pad": {"r": 10, "t": 10},
                    "showactive": False,
                    "type": "buttons",
                    "x": 0.1,
                    "xanchor": "right",
                    "y": 1.15,
                    "yanchor": "top"
                }
            ]
        )

        st.plotly_chart(fig)


#####################################################################################3
        #df_final_filtered
        import plotly.graph_objects as go
        import scipy.stats as stats

        st.subheader("Correlaci√≥n entre el n√∫mero de publicaciones y el n√∫mero de citas.")
        st.markdown(
    """
    <div style='text-align: justify'>
        En esta secci√≥n se muestra la grafica de 
<strong>correlaci√≥n entre las publicaciones y el n√∫mero de citas</strong> para todos los autores registrados en la base. Cada punto corresponde a un autor y si deja el cursor sobre un punto en espec√≠fico, se desplegar√°n los datos del autor al que corresponde ese punto. La l√≠nea roja representa la l√≠nea de tendencia del ajuste a los datos. El coeficiente de person se muestra en la parte superior izquierda. La gr√°fica es interactiva y puede hacer acercamientos a zonas especificas usando los botones que se muestran en la parte superior derecha al dejar el cursor sobre ella. 
    </div>
    """,
    unsafe_allow_html=True
    )
        
        
        # Convertir a valores num√©ricos (por si hay valores en string)
        df_ucol["Cited_by"] = pd.to_numeric(df_ucol["Cited_by"], errors='coerce')
        df_ucol["Publications"] = pd.to_numeric(df_ucol["Publications"], errors='coerce')

        # Eliminar valores NaN si existen
        #df_correlation = df_ucol[["Cited_by", "Publications", "Author(s)_ID", "Normalized_Author_Name"]].dropna()
        df_correlation = df_ucol[["Cited_by", "Publications", "Folio", "Normalized_Author_Name"]].dropna()

        # Calcular la correlaci√≥n de Pearson
        correlation_coefficient, p_value = stats.pearsonr(df_correlation["Cited_by"], df_correlation["Publications"])

        # Ajustar una l√≠nea de tendencia (regresi√≥n lineal)
        slope, intercept = np.polyfit(df_correlation["Publications"], df_correlation["Cited_by"], 1)
        x_line = np.linspace(df_correlation["Publications"].min(), df_correlation["Publications"].max(), 100)
        y_line = slope * x_line + intercept

        # Crear la gr√°fica de dispersi√≥n con informaci√≥n adicional en el tooltip
        fig_corr = go.Figure()

        # Agregar los puntos de dispersi√≥n con el ID y nombre del autor en el hover
        fig_corr.add_trace(go.Scatter(
            x=df_correlation["Publications"],
            y=df_correlation["Cited_by"],
            mode="markers",
            marker=dict(color="blue", opacity=0.5, size=6),
            #text=df_correlation["Normalized_Author_Name"] + "<br>ID: " + df_correlation["Author(s)_ID"].astype(str),
            text=df_correlation["Normalized_Author_Name"] + "<br>ID: " + df_correlation["Folio"].astype(str),
            hoverinfo="text+x+y",
            name="Datos"
        ))

        # Agregar la l√≠nea de tendencia
        fig_corr.add_trace(go.Scatter(
            x=x_line,
            y=y_line,
            mode="lines",
            line=dict(color="red", width=2),
            name="L√≠nea de tendencia"
        ))

        # Configurar el dise√±o
        fig_corr.update_layout(
            title=f"Correlaci√≥n entre los n√∫meros de publicaciones y citas<br>Coeficiente de Pearson: {correlation_coefficient:.2f}, Valor-p: {p_value:.3f}",
            xaxis_title="N√∫mero Total de Art√≠culos",
            yaxis_title="N√∫mero Total de Citas",
            template="plotly_white"
        )    

        # Mostrar la gr√°fica interactiva
        st.plotly_chart(fig_corr)
#############################################################################################33

        import streamlit as st
        import pandas as pd
        import plotly.express as px
        import numpy as np

        st.subheader("Mapa de dispersi√≥n: antig√ºedad vs. publicaciones")
        st.markdown(
    """
    <div style='text-align: justify'>
        En este diagrama se ha separado la base de datos de autores de la Universidad en 
<strong>rangos de antig√ºedad de cinco a√±os</strong>. Cada circulo representa a un autor. Este gr√°fico permite comparar tanto la producci√≥n de autores que tengan antig√ºedades comparables como entre autores de toda la base. Gracias a esta gr√°fica es facil <strong>identificar a los autores mas productivos dentro de cada rango de antig√ºedad</strong>.
    </div>
    """,
    unsafe_allow_html=True
    )
        
        #df_ucol
        # Convertir a valores num√©ricos
        df_ucol["Cited_by"] = pd.to_numeric(df_ucol["Cited_by"], errors='coerce')
        df_ucol["Publications"] = pd.to_numeric(df_ucol["Publications"], errors='coerce')

        # Convertir Year a string y extraer el primer a√±o de publicaci√≥n
        df_ucol["Year"] = df_ucol["Year"].astype(str)
        df_ucol["First_Year"] = pd.to_numeric(df_ucol["Year"].str.extract(r'(\d{4})')[0], errors='coerce')

        # Calcular la antig√ºedad (a√±os desde la primera publicaci√≥n hasta 2025)
        df_ucol["Seniority"] = 2025 - df_ucol["First_Year"]

        # Filtrar valores v√°lidos
        #df_heatmap = df_ucol[["Seniority", "Publications", "Cited_by", "Author(s)_ID", "Normalized_Author_Name"]].dropna()
        df_heatmap = df_ucol[["Seniority", "Publications", "Cited_by", "Folio", "Normalized_Author_Name"]].dropna()

        # Ajustar el tama√±o de los puntos al cu√°druple
        df_heatmap["Size_Metric"] = df_heatmap["Cited_by"] * 10

        # Crear el scatter heatmap con el tama√±o ajustado
        fig_heatmap = px.scatter(
            df_heatmap,
            x="Seniority",
            y="Publications",
            size="Size_Metric",  # Tama√±o de los puntos escalado
            color="Cited_by",  # Color de los puntos seg√∫n el n√∫mero de citas
            labels={
                "Seniority": "Antig√ºedad (a√±os desde la primera publicaci√≥n)",
                "Publications": "Total de publicaciones",
                "Cited_by": "N√∫mero de citas",
                "Size_Metric": "Citas (escalado)"
            },
            title="Mapa de dispersi√≥n: antig√ºedad vs. publicaciones",
            #hover_data={"Author(s)_ID": True, "Normalized_Author_Name": True, "Cited_by": True},
            hover_data={"Folio": True, "Normalized_Author_Name": True, "Cited_by": True},
            color_continuous_scale="Viridis",
            template="plotly_white"
        )

        # Mostrar la gr√°fica interactiva    
        st.plotly_chart(fig_heatmap)
        st.subheader("Diagramas de caja: antig√ºedad vs. publicaciones")
        st.markdown(
    """
    <div style='text-align: justify'>
        En este diagrama se ha separado la base de datos de autores de la Universidad en <strong>rangos de antig√ºedad de cinco a√±os</strong>. Cada punto representa a un autor. El primer gr√°fico muestra el <strong>n√∫mero de citas vs la antig√ºedad</strong>, mientras que el segundo muestra el <strong>n√∫mero de publicaciones vs la antig√ºedad</strong>.
    </div>
    """,
    unsafe_allow_html=True
    )


    #####################################################################################################3

        import streamlit as st
        import pandas as pd
        import plotly.graph_objects as go
        import numpy as np

        #     Convertir a valores num√©ricos
        df_ucol["Cited_by"] = pd.to_numeric(df_ucol["Cited_by"], errors='coerce')
        df_ucol["Publications"] = pd.to_numeric(df_ucol["Publications"], errors='coerce')

        # Convertir Year a string y extraer el primer a√±o de publicaci√≥n
        df_ucol["Year"] = df_ucol["Year"].astype(str)
        df_ucol["First_Year"] = pd.to_numeric(df_ucol["Year"].str.extract(r'(\d{4})')[0], errors='coerce')

        # Calcular la antig√ºedad (a√±os desde la primera publicaci√≥n hasta 2025)
        df_ucol["Seniority"] = 2025 - df_ucol["First_Year"]

        # Agrupar antig√ºedad en intervalos de 5 a√±os
        df_ucol["Seniority_Group"] = (df_ucol["Seniority"] // 5) * 5

        # Calcular los valores del cuartil 75 (Q3) para definir el filtro
        q3_cited = df_ucol["Cited_by"].quantile(0.75)
        q3_publications = df_ucol["Publications"].quantile(0.75)

        df_ucol["Cited_Above_Q3"] = df_ucol["Cited_by"] > q3_cited
        df_ucol["Publications_Above_Q3"] = df_ucol["Publications"] > q3_publications

        # Crear la figura para citas
        fig_cites = go.Figure()
        fig_cites.add_trace(go.Box(
            x=df_ucol["Seniority_Group"],
            y=df_ucol["Cited_by"],
            boxpoints=False,
            notched=True,
            marker=dict(color="lightblue"),
            name="N√∫mero de Citas"
        ))
        fig_cites.add_trace(go.Scatter(
            x=df_ucol["Seniority_Group"],
            y=df_ucol["Cited_by"],
            mode="markers",
            marker=dict(size=8, opacity=0.8, color="darkblue"),
            name="Todos los Autores",
            #text=df_ucol["Normalized_Author_Name"] + "<br>ID: " + df_ucol["Author(s)_ID"],
            text=df_ucol["Normalized_Author_Name"] + "<br>ID: " + df_ucol["Folio"],
            hoverinfo="text+y"
        ))

        # Crear la figura para publicaciones
        fig_publications = go.Figure()
        fig_publications.add_trace(go.Box(
            x=df_ucol["Seniority_Group"],
            y=df_ucol["Publications"],
            boxpoints=False,
            notched=True,
            marker=dict(color="lightgreen"),
            name="N√∫mero de Publicaciones"
        ))
        fig_publications.add_trace(go.Scatter(
            x=df_ucol["Seniority_Group"],
            y=df_ucol["Publications"],
            mode="markers",
            marker=dict(size=8, opacity=0.8, color="darkgreen"),
            name="Todos los Autores",
            #text=df_ucol["Normalized_Author_Name"] + "<br>ID: " + df_ucol["Author(s)_ID"],
            text=df_ucol["Normalized_Author_Name"] + "<br>ID: " + df_ucol["Folio"],
            hoverinfo="text+y"
        ))

        st.write("**Distribuci√≥n del N√∫mero Total de Citas por Antig√ºedad**")
        st.plotly_chart(fig_cites)

        st.write("**Distribuci√≥n del N√∫mero Total de Publicaciones por Antig√ºedad**")
        st.plotly_chart(fig_publications)

    
###############################################################################################################

        import streamlit as st
        import pandas as pd
        import plotly.express as px
        import plotly.graph_objects as go
        import numpy as np
        from sklearn.preprocessing import StandardScaler
        from sklearn.cluster import AgglomerativeClustering
        from sklearn.manifold import TSNE

        st.subheader("Clustering Jer√°rquico de Autores en funci√≥n de su producci√≥n acad√©mica")
        st.markdown(
    """
    <div style='text-align: justify'>
        En esta secci√≥n se utiliza un algoritmo de <strong>clustering jer√°rquico</strong> para clasificar a los autores, de acuerdo a cuatro par√°metros:
        
        - N√∫mero de publicaciones.
        - N√∫mero de citas.
        - Porcentaje de publicaciones financiadas.
        - Antig√ºedad en la Universidad de Colima.
    </div>
    """,
    unsafe_allow_html=True
    )

        st.markdown(
    """
    <div style='text-align: justify'>
        Se utiliz√≥ la gr√°fica de codo para definir el n√∫mero √≥ptimo de clusters, encontrando que los autores pueden dividirse en <strong>5 clusters distintos</strong>. Para visualizar la distribuci√≥n de los autores en los clusters se utiliz√≥ el <strong>gr√°fico t-SNE</strong> que se muestra debajo. En est gr√°fico se puede observar la cercan√≠a de los clusters, que tan compactos son y su tama√±o relativo, de acuerdo al n√∫mero de autores que los conforman.
    </div>
    """,
    unsafe_allow_html=True
    )

    
        # Convertir a valores num√©ricos
        df_ucol["Cited_by"] = pd.to_numeric(df_ucol["Cited_by"], errors='coerce')
        df_ucol["Publications"] = pd.to_numeric(df_ucol["Publications"], errors='coerce')
        df_ucol["Funded_publications"] = pd.to_numeric(df_ucol["Funded_publications"], errors='coerce')

        # Crear la variable Funding Ratio
        df_ucol["Funding_Ratio"] = df_ucol["Funded_publications"] / df_ucol["Publications"]
        df_ucol["Funding_Ratio"] = df_ucol["Funding_Ratio"].fillna(0)

        # Convertir Year a string y extraer el primer a√±o de publicaci√≥n
        df_ucol["Year"] = df_ucol["Year"].astype(str)
        df_ucol["First_Year"] = pd.to_numeric(df_ucol["Year"].str.extract(r'(\d{4})')[0], errors='coerce')

        # Calcular la antig√ºedad (a√±os desde la primera publicaci√≥n hasta 2025)
        df_ucol["Seniority"] = 2025 - df_ucol["First_Year"]

        # Filtrar valores v√°lidos
        df_valid = df_ucol.dropna(subset=["Publications", "Cited_by", "Seniority", "Funding_Ratio"])[
            ["Publications", "Cited_by", "Seniority", "Funding_Ratio"]
        ]

        # Estandarizar los datos
        scaler = StandardScaler()
        df_scaled = scaler.fit_transform(df_valid)

        # Aplicar Hierarchical Clustering (Agglomerative Clustering)
        num_clusters = 5
        agg_clustering = AgglomerativeClustering(n_clusters=num_clusters, linkage="ward")
        df_ucol.loc[df_valid.index, "Cluster"] = agg_clustering.fit_predict(df_scaled)

        # Aplicar t-SNE para reducir dimensiones
        tsne = TSNE(n_components=2, random_state=42)
        df_ucol.loc[df_valid.index, ["TSNE1", "TSNE2"]] = tsne.fit_transform(df_scaled)

        # Crear la gr√°fica de dispersi√≥n con t-SNE y curvas de nivel
        fig_clusters = px.scatter(
            df_ucol,
            x="TSNE1",
            y="TSNE2",
            color=df_ucol["Cluster"].astype(str),
            title="Visualizaci√≥n t-SNE de Clusters con Cuatro Variables (Hierarchical Clustering)",
            labels={"TSNE1": "Componente t-SNE 1", "TSNE2": "Componente t-SNE 2", "Cluster": "Cluster"},
            hover_data={
                "Author(s)_ID": True,
                "Normalized_Author_Name": True,
                "Seniority": True,
                "Funding_Ratio": True
            },
            template="plotly_white"
        )

        # Agregar curvas de nivel    
        fig_clusters.add_trace(go.Histogram2dContour(
            x=df_ucol["TSNE1"],
            y=df_ucol["TSNE2"],
            colorscale="blues",
            showscale=False
        ))

        st.plotly_chart(fig_clusters)

        st.write("**Integrantes del Cluster 0**")

        # Filtrar los autores que est√°n en el cluster 1.0
        df_cluster_0 = df_ucol[df_ucol["Cluster"] == 0.0]
        # Omitir las columnas "Correspondence_Address" y "Year" en el DataFrame df_cluster_1
        columns_to_exclude = ["Authors", "Author_full_names", "Author(s)_ID", "Correspondence_Address", "Year", "Most_frequent_publisher"]
        df_cluster_0 = df_cluster_0.drop(columns=[col for col in columns_to_exclude if col in df_cluster_0.columns])
        df_cluster_0
        
        # üìÇ **Descargar el archivo procesado**
        csv_data = df_cluster_0.to_csv(index=False).encode('utf-8')
        #st.download_button("**Descargar CSV**", csv_data, "df_cluster_0.csv", "text/csv")



        st.write("**Integrantes del Cluster 1**")
        # Filtrar los autores que est√°n en el cluster 1.0
        df_cluster_1 = df_ucol[df_ucol["Cluster"] == 1.0]
        # Omitir las columnas "Correspondence_Address" y "Year" en el DataFrame df_cluster_1
        columns_to_exclude = ["Authors","Author_full_names", "Author(s)_ID", "Correspondence_Address", "Year", "Most_frequent_publisher"]
        df_cluster_1 = df_cluster_1.drop(columns=[col for col in columns_to_exclude if col in df_cluster_1.columns])
        df_cluster_1
        # üìÇ **Descargar el archivo procesado**
        csv_data = df_cluster_1.to_csv(index=False).encode('utf-8')
        #st.download_button("**Descargar CSV**", csv_data, "df_cluster_1.csv", "text/csv")

        st.write("**Integrantes del Cluster 2**")
        # Filtrar los autores que est√°n en el cluster 1.0
        df_cluster_2 = df_ucol[df_ucol["Cluster"] == 2.0]
        # Omitir las columnas "Correspondence_Address" y "Year" en el DataFrame df_cluster_1
        columns_to_exclude = ["Authors","Author_full_names", "Author(s)_ID", "Correspondence_Address", "Year", "Most_frequent_publisher"]
        df_cluster_2 = df_cluster_2.drop(columns=[col for col in columns_to_exclude if col in df_cluster_2.columns])
        df_cluster_2
        # üìÇ **Descargar el archivo procesado**
        csv_data = df_cluster_2.to_csv(index=False).encode('utf-8')
        #st.download_button("**Descargar CSV**", csv_data, "df_cluster_2.csv", "text/csv")
        

        st.write("**Integrantes del Cluster 3**")
        # Filtrar los autores que est√°n en el cluster 1.0
        df_cluster_3 = df_ucol[df_ucol["Cluster"] == 3.0]
        # Omitir las columnas "Correspondence_Address" y "Year" en el DataFrame df_cluster_1
        columns_to_exclude = ["Authors","Author_full_names", "Author(s)_ID", "Correspondence_Address", "Year", "Most_frequent_publisher"]
        df_cluster_3 = df_cluster_3.drop(columns=[col for col in columns_to_exclude if col in df_cluster_3.columns])
        df_cluster_3
        # üìÇ **Descargar el archivo procesado**
        csv_data = df_cluster_3.to_csv(index=False).encode('utf-8')
        #st.download_button("**Descargar CSV**", csv_data, "df_cluster_3.csv", "text/csv")

        st.write("**Integrantes del Cluster 4**")
        # Filtrar los autores que est√°n en el cluster 1.0
        df_cluster_4 = df_ucol[df_ucol["Cluster"] == 4.0]
        # Omitir las columnas "Correspondence_Address" y "Year" en el DataFrame df_cluster_1
        columns_to_exclude = ["Authors","Author_full_names", "Author(s)_ID", "Correspondence_Address", "Year", "Most_frequent_publisher"]
        df_cluster_4 = df_cluster_4.drop(columns=[col for col in columns_to_exclude if col in df_cluster_4.columns])
        df_cluster_4
        # üìÇ **Descargar el archivo procesado**
        csv_data = df_cluster_4.to_csv(index=False).encode('utf-8')
        #st.download_button("**Descargar CSV**", csv_data, "df_cluster_4.csv", "text/csv")

        # Contar la cantidad de registros en cada cluster
        cluster_counts = df_ucol["Cluster"].value_counts().sort_index()

        # Calcular el porcentaje de cada cluster respecto al total
        total_count = len(df_ucol)
        cluster_percentages = (cluster_counts / total_count) * 100

        # Crear etiquetas con el total de autores por cluster
        labels = [f"Cluster {i} ({count} autores)" for i, count in cluster_counts.items()]

        # Crear gr√°fico de pastel con leyenda de total de autores
        fig_pie = go.Figure(data=[
            go.Pie(labels=labels, values=cluster_percentages, textinfo='label+percent', hole=0.4)
        ])
        fig_pie.update_layout(title_text=f"N√∫mero de autores clasificados en cada cluster. Total de autores: {total_count}")

        #st.write("**El siguiente gr√°fico muestra el porcentaje de publicaciones que aporta cada cluster.**")
        st.plotly_chart(fig_pie)


    ##################################################################################3

        import streamlit as st
        import pandas as pd
        import plotly.express as px
        import plotly.graph_objects as go
        import numpy as np
        from sklearn.preprocessing import StandardScaler
        from sklearn.cluster import AgglomerativeClustering
        from sklearn.manifold import TSNE
        import plotly.colors as pc

        #st.title("üìä Clustering Jer√°rquico de Autores en Funci√≥n de su Producci√≥n Acad√©mica")

        st.markdown("""
        Los siguientes gr√°ficos comparan a los clusters en las cuatro variables clave. De arriba a abajo estas son: el n√∫mero de publicaciones, el de citas, el porcentaje de publicaciones financiadas y la antig√ºedad.
        """)
        
        # Convertir a valores num√©ricos
        df_ucol["Cited_by"] = pd.to_numeric(df_ucol["Cited_by"], errors='coerce')
        df_ucol["Publications"] = pd.to_numeric(df_ucol["Publications"], errors='coerce')
        df_ucol["Funded_publications"] = pd.to_numeric(df_ucol["Funded_publications"], errors='coerce')
        df_ucol["Seniority"] = pd.to_numeric(df_ucol["Seniority"], errors='coerce')

        # Crear la variable Funding Ratio
        df_ucol["Funding_Ratio"] = df_ucol["Funded_publications"] / df_ucol["Publications"]
        df_ucol["Funding_Ratio"] = df_ucol["Funding_Ratio"].fillna(0)

        # Eliminar √≠ndices duplicados si existen
        df_ucol = df_ucol.drop_duplicates()

        # Filtrar solo las columnas necesarias para el an√°lisis    
        df_boxplot = df_ucol[["Cluster", "Publications", "Cited_by", "Seniority", "Funding_Ratio"]].copy()

        # Convertir "Cluster" a tipo categ√≥rico
        df_boxplot["Cluster"] = df_boxplot["Cluster"].astype(str)
    
        # Definir colores de los clusters extra√≠dos de la visualizaci√≥n t-SNE
        cluster_colors = pc.qualitative.Plotly[:len(df_boxplot["Cluster"].unique())]
        color_mapping = {str(cluster): color for cluster, color in zip(df_boxplot["Cluster"].unique(), cluster_colors)}

        # Crear los diagramas de caja con los colores de los clusters
        fig1 = px.box(df_boxplot, x="Cluster", y="Publications", color="Cluster",
              title="N√∫mero de Publicaciones por Cluster",
              labels={"Cluster": "Cluster", "Publications": "N√∫mero de publicaciones"},
              notched=True, template="plotly_white",
              color_discrete_map=color_mapping)

        fig2 = px.box(df_boxplot, x="Cluster", y="Cited_by", color="Cluster",
              title="N√∫mero de Citas por Cluster",
              labels={"Cluster": "Cluster", "Cited_by": "N√∫mero de citas"},
              notched=True, template="plotly_white",
              color_discrete_map=color_mapping)

        fig3 = px.box(df_boxplot, x="Cluster", y="Funding_Ratio", color="Cluster",
              title="Cociente de Publicaciones Financiadas por Cluster",
              labels={"Cluster": "Cluster", "Funding_Ratio": "Proporci√≥n de publicaciones financiadas"},
              notched=True, template="plotly_white",
              color_discrete_map=color_mapping)

        fig4 = px.box(df_boxplot, x="Cluster", y="Seniority", color="Cluster",
              title="Antig√ºedad por Cluster",
              labels={"Cluster": "Cluster", "Seniority": "A√±os desde la primera publicaci√≥n"},
              notched=True, template="plotly_white",
              color_discrete_map=color_mapping)

        # Mostrar las gr√°ficas en Streamlit
        st.plotly_chart(fig1)
        st.plotly_chart(fig2)
        st.plotly_chart(fig3)
        st.plotly_chart(fig4)

    #####################################################################################333

        import pandas as pd
        import numpy as np
        import matplotlib.pyplot as plt
        import plotly.express as px
        from sklearn.model_selection import train_test_split
        from sklearn.tree import DecisionTreeClassifier, plot_tree
        from sklearn.metrics import classification_report, confusion_matrix
        import seaborn as sns

        
        # üèÜ **√Årbol de Decisi√≥n para Predicci√≥n de Clusters**
        st.header("Obtenci√≥n de reglas de clasificaci√≥n y puntos de corte a partir de un modelo de √°rbol de decisi√≥n")
        st.markdown("""En esta secci√≥n se usa un modelo de √°rbol de decisi√≥n para obtener las reglas que permiten clasificar a los autores dentro de cada cluster. Gracias a esto, es posible generar un modelo que permita clasificar a nuevos autores en cada cluster, obtener un perfil para cada uno que permita identificar el nivel de madurez y productividad que los caracteriza y simplificar la identificaci√≥n de posibles l√≠neas de acci√≥n para impulsar la producci√≥n cient√≠fica en la Universidad de Colima. Puede ver las matrices de confusi√≥n y otris instrumentos de validaci√≥n de este model si da click al bot√≥n "Validaci√≥n del modelo". """)
        # Filtrar datos v√°lidos
        df_valid = df_ucol.dropna(subset=["Funding_Ratio", "Publications", "Cited_by", "Seniority", "Cluster"])

        # Definir variables predictoras y objetivo
        X = df_valid[["Funding_Ratio", "Publications", "Cited_by", "Seniority"]]

        # Crear un diccionario para asignar los clusters originales a etiquetas ordenadas
        cluster_mapping = {cluster: idx for idx, cluster in enumerate(sorted(df_valid["Cluster"].unique()))}
        reverse_mapping = {v: k for k, v in cluster_mapping.items()}  # Para revertir la codificaci√≥n

        # Reemplazar los valores originales por los √≠ndices ordenados
        y = df_valid["Cluster"].map(cluster_mapping).astype(int)

        # Dividir en conjunto de entrenamiento y prueba
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

        # Entrenar el modelo de √Årbol de Decisi√≥n
        clf = DecisionTreeClassifier(random_state=42, max_depth=4)
        clf.fit(X_train, y_train)

        # Evaluar el modelo
        y_pred = clf.predict(X_test)

        # Reconvertir las predicciones y etiquetas originales a los valores reales de cluster
        y_test_original = y_test.map(reverse_mapping)
        y_pred_original = pd.Series(y_pred).map(reverse_mapping)


        with st.expander("**Validaci√≥n del modelo**"):
        # Matriz de confusi√≥n
            st.subheader("Matriz de Confusi√≥n")
            ## üìå Matriz de Confusi√≥n
            st.markdown(""" 
            Muestra los aciertos y errores del modelo de clasificaci√≥n comparando predicciones con valores reales.
            
            - TP (True Positive): Predicciones correctas de la clase positiva.
            - FP (False Positive): Casos incorrectamente clasificados como positivos.
            - FN (False Negative): Casos incorrectamente clasificados como negativos.
            - TN (True Negative): Predicciones correctas de la clase negativa.
            """)

            st.write(pd.DataFrame(confusion_matrix(y_test_original, y_pred_original),
                      index=[f"Actual {reverse_mapping[c]}" for c in sorted(y.unique())],
                      columns=[f"Predicho {reverse_mapping[c]}" for c in sorted(y.unique())]))

            # Reporte de Clasificaci√≥n
            st.subheader("Reporte de Clasificaci√≥n")
            st.markdown("""
            Resumen de m√©tricas clave del modelo:

            - Precisi√≥n: Proporci√≥n de predicciones correctas en cada clase.
            - Recall: Capacidad del modelo para detectar todos los casos positivos.
            - F1-score: Media arm√≥nica entre precisi√≥n y recall.
            """)
                         
            st.text(classification_report(y_test_original, y_pred_original))

            # Importancia de las Variables
            st.subheader("Importancia de las Variables en el Modelo")
            st.markdown("""
            Muestra cu√°nto influye cada variable en las predicciones del modelo.
            """)
            importances = pd.Series(clf.feature_importances_, index=X.columns)
            fig_importance = px.bar(importances, x=importances.index, y=importances.values,
                        labels={"x": "Variables", "y": "Importancia Relativa"},
                        title="Importancia de las Variables en el √Årbol de Decisi√≥n",
                        template="plotly_white")
            st.plotly_chart(fig_importance)

        # Visualizaci√≥n del √Årbol de Decisi√≥n
        st.subheader("Modelo de √Årbol de decisi√≥n")
        fig, ax = plt.subplots(figsize=(30, 15))
        plot_tree(clf, feature_names=X.columns, class_names=[str(reverse_mapping[c]) for c in sorted(y.unique())],
                  filled=True, fontsize=8, ax=ax)
        st.pyplot(fig)



        import streamlit as st
        import pandas as pd
        import plotly.express as px
        import plotly.graph_objects as go
        import numpy as np
        from sklearn.preprocessing import StandardScaler
        from sklearn.cluster import AgglomerativeClustering
        from sklearn.manifold import TSNE
        import plotly.colors as pc
        from sklearn.model_selection import train_test_split
        from sklearn.tree import DecisionTreeClassifier, plot_tree
        from sklearn.metrics import classification_report, confusion_matrix
        import seaborn as sns
        import matplotlib.pyplot as plt

        # Convertir a valores num√©ricos
        df_ucol["Cited_by"] = pd.to_numeric(df_ucol["Cited_by"], errors='coerce')
        df_ucol["Publications"] = pd.to_numeric(df_ucol["Publications"], errors='coerce')
        df_ucol["Funded_publications"] = pd.to_numeric(df_ucol["Funded_publications"], errors='coerce')
        df_ucol["Seniority"] = pd.to_numeric(df_ucol["Seniority"], errors='coerce')

        # Crear la variable Funding Ratio
        df_ucol["Funding_Ratio"] = df_ucol["Funded_publications"] / df_ucol["Publications"]
        df_ucol["Funding_Ratio"] = df_ucol["Funding_Ratio"].fillna(0)

        # Filtrar datos v√°lidos
        df_valid = df_ucol.dropna(subset=["Funding_Ratio", "Publications", "Cited_by", "Seniority", "Cluster"])

        # Definir variables predictoras y objetivo
        X = df_valid[["Funding_Ratio", "Publications", "Cited_by", "Seniority"]]

        # Crear un diccionario para asignar los clusters originales a etiquetas ordenadas
        cluster_mapping = {cluster: idx for idx, cluster in enumerate(sorted(df_valid["Cluster"].unique()))}
        reverse_mapping = {v: k for k, v in cluster_mapping.items()}  # Para revertir la codificaci√≥n

        # Reemplazar los valores originales por los √≠ndices ordenados    
        y = df_valid["Cluster"].map(cluster_mapping).astype(int)

        # Entrenar el modelo de √Årbol de Decisi√≥n
        clf = DecisionTreeClassifier(random_state=42, max_depth=4)
        clf.fit(X, y)

        # üìå **Formulario Inteligente para Asignaci√≥n de Cluster**
        st.header("üìù Predicci√≥n de Cluster Basado en Estad√≠sticas de Autor")

        st.markdown("""
        En el siguiente formulario es posible que el usuario introduzca sus datos y obtenga su clasificaci√≥n dentro de alguno de los clusters de autores descritos arriba.
        """)


        # Inicializar valores en session_state solo si no existen
        if "funding_ratio" not in st.session_state:
            st.session_state.funding_ratio = 0.0
        if "publications" not in st.session_state:
            st.session_state.publications = 0
        if "cited_by" not in st.session_state:
            st.session_state.cited_by = 0
        if "seniority" not in st.session_state:
            st.session_state.seniority = 0
        if "predicted_cluster" not in st.session_state:  # <-- Inicializar el cluster
            st.session_state.predicted_cluster = "Sin asignar"
        
        # Inicializar valores en session_state solo si no existen
        #if "funding_ratio" not in st.session_state:
        #    st.session_state.funding_ratio = 0.0
        #if "publications" not in st.session_state:
        #    st.session_state.publications = 0
        #if "cited_by" not in st.session_state:
        #    st.session_state.cited_by = 0
        #if "seniority" not in st.session_state:
        #    st.session_state.seniority = 0

        # Campos de entrada con valores persistentes usando `key`
        funding_ratio = st.number_input("**Proporci√≥n de publicaciones financiadas**", 
                                min_value=0.0, max_value=1.0, step=0.01, 
                                value=st.session_state.funding_ratio, key="funding_ratio")

        publications = st.number_input("**N√∫mero de publicaciones**", 
                               min_value=0, step=1, 
                               value=st.session_state.publications, key="publications")

        cited_by = st.number_input("**N√∫mero de citas**", 
                           min_value=0, step=1, 
                           value=st.session_state.cited_by, key="cited_by")

        seniority = st.number_input("**Antig√ºedad (a√±os desde la primera publicaci√≥n)**", 
                            min_value=0, max_value=100, step=1, 
                            value=st.session_state.seniority, key="seniority")

          # Inicializar valores en session_state solo si no existen
        if "predicted_cluster" not in st.session_state:
            st.session_state.predicted_cluster = None  # Se inicializa con None

        # Bot√≥n para asignar cluster
        if st.button("**Asignar Cluster**"):
            user_data = np.array([[st.session_state.funding_ratio, 
                           st.session_state.publications, 
                           st.session_state.cited_by, 
                           st.session_state.seniority]])

            predicted_cluster_idx = clf.predict(user_data)[0]
            st.session_state.predicted_cluster = str(int(reverse_mapping[predicted_cluster_idx]))  # Guardar en session_state

            st.success(f"**Has sido asignado al Cluster {st.session_state.predicted_cluster}**")
                        # Explicaci√≥n basada en el perfil de publicaciones
            cluster_explanations = {
                "0": "Autores con baja producci√≥n y pocas citas, posiblemente en inicio de carrera. "
                     "Suelen tener una antig√ºedad variable, pero con baja producci√≥n en publicaciones y un impacto limitado en citas. "
                     "El financiamiento es bajo o moderado. Estos autores pueden estar comenzando su trayectoria o no enfocarse completamente en la investigaci√≥n.",

                "1": "Autores con una trayectoria consolidada, con muchas publicaciones y alta citaci√≥n. "
                     "Son investigadores de alto impacto, con un gran n√∫mero de publicaciones y citas. "
                     "Suelen tener una antig√ºedad alta en la academia (> 9.5 a√±os) y financiamiento moderado o alto. "
                     "Frecuentemente tienen colaboraciones internacionales y publican en revistas de alto impacto.",

                "2": "Autores con producci√≥n moderada y algunas citas, con crecimiento acad√©mico estable. "
                     "Estos investigadores tienen una producci√≥n media en publicaciones y citas. "
                     "Suelen contar con financiamiento moderado y m√°s de 9.5 a√±os de antig√ºedad en la academia. "
                     "Es un perfil t√≠pico de acad√©micos en consolidaci√≥n o en √°reas emergentes con un crecimiento estable en citas.",

                "3": "Autores en inicio de carrera con baja producci√≥n. "
                     "Tienen pocos a√±os en la academia (‚â§ 9.5 a√±os), pocas publicaciones y bajo impacto en citas. "
                     "El financiamiento es bajo o nulo, y su producci√≥n a√∫n no ha crecido significativamente. "
                     "Pueden ser investigadores j√≥venes, profesores con menor enfoque en investigaci√≥n o estudiantes de doctorado.",

                "4": "Autores con producci√≥n alta y financiamiento significativo. "
                     "Son l√≠deres en investigaci√≥n con m√∫ltiples proyectos financiados. "
                     "Tienen una producci√≥n establecida en revistas de alto impacto y acceso a financiamiento significativo. "
                     "Suelen tener una antig√ºedad alta, con equipos de trabajo consolidados y un alto impacto en citas."
            }

            st.info(cluster_explanations.get(st.session_state.predicted_cluster, "Descripci√≥n no disponible.")) 

        if st.session_state.predicted_cluster is not None and st.session_state.predicted_cluster.isdigit():
            df_cluster = df_valid[df_valid["Cluster"] == int(st.session_state.predicted_cluster)]
            
        else:
            df_cluster = pd.DataFrame()  # Evita error si el cluster no es un n√∫mero

        if st.session_state.predicted_cluster and str(st.session_state.predicted_cluster).isdigit():
            cluster_id = int(st.session_state.predicted_cluster)
            df_cluster = df_valid[df_valid["Cluster"] == cluster_id]
    
            st.markdown("### Comparativa con tu Cluster")

            # Ejemplo: mostrar estad√≠sticas comparativas
            #st.dataframe(df_cluster.describe())  # Solo como ejemplo
        
            st.markdown("""
            En la siguiente tabla puede verse la comparaci√≥n entre los datos especificados por el usuario, los integrantes del Cluster que guarda la mayor similitud con estos datos y con los autores de la base de datos completa. Las columnas de inter√©s son: el par√°metro a comparar (ya sea el n√∫mero de publicaciones, citas, la antig√ºedad o la porporci√≥n de publicaciones financiadas), los datos introducidos por el usuario, la media, el cuartil 1 y el cuartil 3 del cluster que corresponda, las medias, cuartil 1 y cuartil 3 de la base completa.
        """)
            # üìå **Filtrar Datos del Cluster y Crear DataFrame del Usuario**
            df_cluster = df_valid[df_valid["Cluster"] == int(st.session_state.predicted_cluster)]

            df_user = pd.DataFrame({
                "M√©trica": ["Publications", "Cited_by", "Seniority", "Funding_Ratio"],
                "Valor del Usuario": [
                    st.session_state.publications, 
                    st.session_state.cited_by, 
                    st.session_state.seniority, 
                    st.session_state.funding_ratio
                ]
            })

            # üìå **Calcular Estad√≠sticas**
            comparison_data = {
                "M√©trica": ["Publications", "Cited_by", "Seniority", "Funding_Ratio"],
                "Valor del Usuario": [
                    st.session_state.publications, 
                    st.session_state.cited_by, 
                    st.session_state.seniority, 
                    st.session_state.funding_ratio
                ],

                # üìå **Estad√≠sticas del Cluster**
                "Cluster - Media": [
                    df_cluster["Publications"].mean(), df_cluster["Cited_by"].mean(),
                    df_cluster["Seniority"].mean(), df_cluster["Funding_Ratio"].mean()
                ],
                "Cluster - Q1 (P25)": [
                    df_cluster["Publications"].quantile(0.25), df_cluster["Cited_by"].quantile(0.25),
                    df_cluster["Seniority"].quantile(0.25), df_cluster["Funding_Ratio"].quantile(0.25)
                ],
                "Cluster - Q3 (P75)": [
                    df_cluster["Publications"].quantile(0.75), df_cluster["Cited_by"].quantile(0.75),
                    df_cluster["Seniority"].quantile(0.75), df_cluster["Funding_Ratio"].quantile(0.75)
                ],

                # üìå **Estad√≠sticas de la Base Completa**
                "Base - Media": [
                    df_valid["Publications"].mean(), df_valid["Cited_by"].mean(),
                    df_valid["Seniority"].mean(), df_valid["Funding_Ratio"].mean()
                ],
                "Base - Q1 (P25)": [
                    df_valid["Publications"].quantile(0.25), df_valid["Cited_by"].quantile(0.25),
                    df_valid["Seniority"].quantile(0.25), df_valid["Funding_Ratio"].quantile(0.25)
                ],
                "Base - Q3 (P75)": [
                    df_valid["Publications"].quantile(0.75), df_valid["Cited_by"].quantile(0.75),
                    df_valid["Seniority"].quantile(0.75), df_valid["Funding_Ratio"].quantile(0.75)
                ]
            }

            # üìå **Convertir a DataFrame**
            df_comparison = pd.DataFrame(comparison_data)

            # üìå **Mostrar la Tabla en Streamlit**
            st.dataframe(df_comparison.style.format({
                "Valor del Usuario": "{:.2f}",
                "Cluster - Media": "{:.2f}", "Cluster - Q1 (P25)": "{:.2f}", "Cluster - Q3 (P75)": "{:.2f}",
                "Base - Media": "{:.2f}", "Base - Q1 (P25)": "{:.2f}", "Base - Q3 (P75)": "{:.2f}"
            }))

            st.markdown("""
            Debajo, puede ver las graficas de caja entre los datos registrados para el cluster (primeras cuatro gr√°ficas), la base completa (segundo grupo de cuatro gr√°ficas) y los                     valores introducidos por el usuario.
            """)

#############################################################################################################


            # üìå **Definir Funci√≥n para Graficar Comparaciones**
            def plot_comparison(metric, title, y_label):
                fig_cluster = px.box(df_cluster, y=metric, points="all", 
                         title=f"{title} en el Cluster {st.session_state.predicted_cluster}",
                         labels={metric: y_label},
                         template="plotly_white")
    
                fig_cluster.add_trace(go.Scatter(
                    x=["Usuario"], y=[df_user[df_user["M√©trica"] == metric]["Valor del Usuario"].values[0]], 
                    mode="markers+text", text="üìç", textposition="top center",
                    marker=dict(color="red", size=12),
                    name="Usuario"
                ))
    
                fig_base = px.box(df_valid, y=metric, points="all", 
                      title=f"{title} en Toda la Base",
                      labels={metric: y_label},
                      template="plotly_white")
    
                fig_base.add_trace(go.Scatter(
                    x=["Usuario"], y=[df_user[df_user["M√©trica"] == metric]["Valor del Usuario"].values[0]], 
                    mode="markers+text", text="üìç", textposition="top center",
                    marker=dict(color="red", size=12),
                    name="Usuario"
                ))
    
                return fig_cluster, fig_base

            # üìå **Comparaciones por M√©trica**
            fig_pub_cluster, fig_pub_base = plot_comparison("Publications", "N√∫mero de Publicaciones", "Publicaciones")
            fig_cite_cluster, fig_cite_base = plot_comparison("Cited_by", "N√∫mero de Citas", "Citas")
            fig_sen_cluster, fig_sen_base = plot_comparison("Seniority", "Antig√ºedad", "A√±os desde la Primera Publicaci√≥n")
            fig_fund_cluster, fig_fund_base = plot_comparison("Funding_Ratio", "Proporci√≥n de Publicaciones Financiadas", "Ratio de Financiamiento")

            # üìå **Mostrar Gr√°ficos**
            st.subheader(f"üìä Comparaci√≥n con Autores del Cluster {st.session_state.predicted_cluster}")
            st.plotly_chart(fig_pub_cluster)
            st.plotly_chart(fig_cite_cluster)
            st.plotly_chart(fig_sen_cluster)
            st.plotly_chart(fig_fund_cluster)

            st.subheader("üìä Comparaci√≥n con Toda la Base de Datos")
            st.plotly_chart(fig_pub_base)    
            st.plotly_chart(fig_cite_base)
            st.plotly_chart(fig_sen_base)
            st.plotly_chart(fig_fund_base)


        else:
            st.warning("üïµÔ∏è‚Äç‚ôÇÔ∏è Por favor completa el formulario y haz clic en **Asignar Cluster** para ver resultados.")
    else:
        st.info("üìÇ **Sube un archivo CSV para comenzar**")


elif pagina == "An√°lisis de temas por √°rea":
        

    import pandas as pd
    from sklearn.feature_extraction.text import TfidfVectorizer
    from sklearn.svm import SVC
    from sklearn.pipeline import Pipeline
    from sklearn.model_selection import train_test_split
    from sklearn.metrics import classification_report

    # Cargar el archivo CSV
    file_path = "scopusUdeC con financiamiento 17 feb-2.csv"

    #file_path = "/mnt/data/scopusUdeC con financiamiento 17 feb-2.csv"
    df = pd.read_csv(file_path, encoding='utf-8')
    #df_ucol
    # Diccionario extendido de palabras clave por √°rea tem√°tica
    area_mapping_extended = {
            "F√≠sica y Matem√°ticas": ["Physical Review", "Mathematics", "Quantum", "Astrophysics", "Topology"],
            "Qu√≠mica": ["ChemEngineering", "Pharmaceuticals", "Chemical", "Biochemistry", "Catalysis"],
            "Ingenier√≠a": ["Engineering", "Robotics", "Technology", "Automation", "Materials Science"],
            "Medicina": ["Medicine", "Oncology", "Neurology", "Public Health", "Epidemiology"],
            "Biolog√≠a": ["Biology", "Microbiology", "Genomics", "Ecology", "Botany"],
            "Humanidades": ["Social Science", "History", "Philosophy", "Education", "Sociology"]
        }

    # Funci√≥n para asignar un √°rea tem√°tica basada en palabras clave
    def assign_area_extended_v2(row):
        source_title = str(row["Source title"])
        title = str(row["Title"])
    
        for area, keywords in area_mapping_extended.items():
            if any(keyword in source_title for keyword in keywords) or any(keyword in title for keyword in keywords):
                return area
        return "Otras"

    # Aplicar clasificaci√≥n inicial
    df["√Årea Tem√°tica"] = df.apply(assign_area_extended_v2, axis=1)

    # Filtrar solo art√≠culos con √°rea tem√°tica definida
    df_labeled = df[df["√Årea Tem√°tica"] != "Otras"]

    # Datos de entrenamiento y prueba
    X = df_labeled["Title"].astype(str)
    y = df_labeled["√Årea Tem√°tica"]
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

    # Modelo SVM con mejor preprocesamiento
    vectorizer = TfidfVectorizer(stop_words="english", ngram_range=(1,2), max_features=5000)
    model_svm = Pipeline([
        ("vectorizer", vectorizer),
        ("classifier", SVC(kernel="linear", probability=True))
    ])

    # Entrenar el modelo
    model_svm.fit(X_train, y_train)

    # Evaluar el modelo
    y_pred_svm = model_svm.predict(X_test)
    print(classification_report(y_test, y_pred_svm))

    # Aplicar el modelo a los art√≠culos en "Otras"
    df_otros = df[df["√Årea Tem√°tica"] == "Otras"].copy()
    df_otros["√Årea Tem√°tica"] = model_svm.predict(df_otros["Title"].astype(str))

    # Actualizar la base de datos
    df.update(df_otros)
    #df
    # Guardar archivo procesado
    df.to_csv("scopus_procesado.csv", index=False, encoding='utf-8')
    #st.download_button("Descargar Base Procesada", "scopus_procesado.csv")

    
    import streamlit as st
    import pandas as pd
    import os
    from wordcloud import WordCloud
    import matplotlib.pyplot as plt
    from sklearn.feature_extraction.text import TfidfVectorizer        
    from sklearn.svm import SVC
    from sklearn.pipeline import Pipeline
    from sklearn.model_selection import train_test_split

    # Configuraci√≥n de la aplicaci√≥n en Streamlit
    #st.title("An√°lisis de √Åreas Tem√°ticas y Nubes de Palabras")

##################################################################################

    import streamlit as st
    import pandas as pd
    import matplotlib.pyplot as plt
    from wordcloud import WordCloud
    import nltk
    from nltk.corpus import stopwords
    import string
    import re

#        # Descargar stopwords si es la primera vez ejecutando el c√≥digo
#        nltk.download("stopwords")

#        # Lista adicional de palabras comunes a excluir (convertidas a min√∫sculas para evitar problemas de coincidencia)
#        custom_stopwords = {word.lower() for word in [
#            "study", "method", "analysis", "model", "data", "results", "research", "approach", 
#            "colima", "mexico", "asses", "assessment", "design", "mexican", "c√≥mo", "using", 
#            "partial", "m√©xico", "effect", "comment", "based", "central", "evaluation", "employing", 
#            "transformation", "application", "system", "approach", "n", "effects"]}

#        # Configuraci√≥n de la aplicaci√≥n en Streamlit
#        st.title("An√°lisis de √Åreas Tem√°ticas y Nubes de Palabras")

#        # Diccionario extendido de palabras clave por √°rea tem√°tica
#        area_mapping_extended = {
#        "F√≠sica y Matem√°ticas": ["Physical Review", "Mathematics", "Quantum", "Astrophysics", "Topology"],
#        "Qu√≠mica": ["ChemEngineering", "Pharmaceuticals", "Chemical", "Biochemistry", "Catalysis"],
#        "Ingenier√≠a": ["Engineering", "Robotics", "Technology", "Automation", "Materials Science"],
#        "Medicina": ["Medicine", "Oncology", "Neurology", "Public Health", "Epidemiology"],
#        "Biolog√≠a": ["Biology", "Microbiology", "Genomics", "Ecology", "Botany"],
#        "Humanidades": ["Social Science", "History", "Philosophy", "Education", "Sociology"]
#        }

#        # Funci√≥n para asignar un √°rea tem√°tica
#        def assign_area_extended_v2(row):
#            source_title = str(row["Source title"])
#            title = str(row["Title"])
    
#            for area, keywords in area_mapping_extended.items():
#                if any(keyword in source_title for keyword in keywords) or any(keyword in title for keyword in keywords):
#                    return area
#            return "Otras"

#        # Aplicar clasificaci√≥n inicial
#        df["√Årea Tem√°tica"] = df.apply(assign_area_extended_v2, axis=1)

#        # Entrenar el modelo SVM si hay datos etiquetados
#        df_labeled = df[df["√Årea Tem√°tica"] != "Otras"]
#        if not df_labeled.empty:
#            X = df_labeled["Title"].astype(str)
#            y = df_labeled["√Årea Tem√°tica"]
#            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

#            # Modelo SVM
#            vectorizer = TfidfVectorizer(stop_words="english", ngram_range=(1,2), max_features=5000)
#            model_svm = Pipeline([
#                ("vectorizer", vectorizer),
#                ("classifier", SVC(kernel="linear", probability=True))
#            ])

#            model_svm.fit(X_train, y_train)
#            df_otros = df[df["√Årea Tem√°tica"] == "Otras"].copy()
#            df_otros["√Årea Tem√°tica"] = model_svm.predict(df_otros["Title"].astype(str))
#            df.update(df_otros)

#        # Funci√≥n para generar nubes de palabras con stopwords eliminadas
#        def generar_nubes_palabras(df):
#            st.subheader("Nubes de Palabras por √Årea Tem√°tica")
#            a√±os_disponibles = sorted(df["Year"].dropna().unique(), reverse=True)[:8]
#            areas_interes = ["F√≠sica y Matem√°ticas", "Qu√≠mica", "Ingenier√≠a", "Medicina", "Biolog√≠a", "Humanidades"]

#            stop_words = set(stopwords.words("english")) | set(stopwords.words("spanish")) | set(string.punctuation) | custom_stopwords
    
#            def limpiar_texto(texto):
#                texto = texto.lower()
#                texto = re.sub(r"[\W_]+", " ", texto)  # Remover puntuaci√≥n y caracteres especiales
#                palabras = texto.split()
#                palabras_filtradas = [word for word in palabras if word not in stop_words and len(word) > 2]
#                return " ".join(palabras_filtradas)

#            for a√±o in a√±os_disponibles:
#                df_a√±o = df[df["Year"] == a√±o]
#                if df_a√±o.empty:
#                    continue

#                st.subheader(f"A√±o {a√±o}")
#                fig, axes = plt.subplots(2, 3, figsize=(18, 12))
#                axes = axes.flatten()

#                for i, area in enumerate(areas_interes):
#                    df_area = df_a√±o[df_a√±o["√Årea Tem√°tica"] == area]
#                    if not df_area.empty:
#                        text = " ".join(df_area["Title"].dropna())
#                        filtered_text = limpiar_texto(text)
#                        wordcloud = WordCloud(width=800, height=400, background_color="white").generate(filtered_text)
#                        axes[i].imshow(wordcloud, interpolation="bilinear")
#                        axes[i].set_title(f"{area} ({a√±o})", fontsize=14)
#                        axes[i].axis("off")
#                    else:
#                        axes[i].axis("off")

#                plt.tight_layout()
#                st.pyplot(fig)

#        # Generar nubes autom√°ticamente sin necesidad de bot√≥n
#        generar_nubes_palabras(df)
###################################################################################
###################################################################################

    import streamlit as st
    import pandas as pd
    import matplotlib.pyplot as plt
    from wordcloud import WordCloud
    import nltk
    from nltk.corpus import stopwords
    from nltk.stem import WordNetLemmatizer
    from deep_translator import GoogleTranslator
    import string
    import re
    
    # Descargar recursos de NLTK
    nltk.download("stopwords")
    nltk.download("wordnet")
    nltk.download("omw-1.4")

    # Inicializar lematizador y traductor
    lemmatizer = WordNetLemmatizer()
        #translator = GoogleTranslator(source='auto', target='english')  # Traducir todo a ingl√©s

    # Lista adicional de palabras comunes a excluir (convertidas a min√∫sculas para evitar problemas de coincidencia)
    custom_stopwords = {word.lower() for word in [
            "study", "method", "analysis", "model", "data", "results", "research", "approach", 
            "colima", "mexico", "asses", "assessment", "design", "mexican", "c√≥mo", "using", 
            "partial", "m√©xico", "effect", "comment", "based", "central", "evaluation", "employing", 
            "transformation", "application", "system", "approach", "n", "effects", "one", "two", "low", "high", "2021", "2020", "2019", "2022", "2018", "2017", "fast", "slow", "large", "small", ]}

    # Configuraci√≥n de la aplicaci√≥n en Streamlit
    st.title("An√°lisis de √Åreas Tem√°ticas y Nubes de Palabras")

    # Diccionario extendido de palabras clave por √°rea tem√°tica
    area_mapping_extended = {
            "F√≠sica y Matem√°ticas": ["Physical Review", "Mathematics", "Quantum", "Astrophysics", "Topology"],
            "Qu√≠mica": ["ChemEngineering", "Pharmaceuticals", "Chemical", "Biochemistry", "Catalysis"],
            "Ingenier√≠a": ["Engineering", "Robotics", "Technology", "Automation", "Materials Science"],
            "Medicina": ["Medicine", "Oncology", "Neurology", "Public Health", "Epidemiology"],
            "Biolog√≠a": ["Biology", "Microbiology", "Genomics", "Ecology", "Botany"],
            "Humanidades": ["Social Science", "History", "Philosophy", "Education", "Sociology"]
    }

    # Funci√≥n para asignar un √°rea tem√°tica
    def assign_area_extended_v2(row):
        source_title = str(row["Source title"])
        title = str(row["Title"])
    
        for area, keywords in area_mapping_extended.items():
            if any(keyword in source_title for keyword in keywords) or any(keyword in title for keyword in keywords):
                return area
        return "Otras"

    # Aplicar clasificaci√≥n inicial
    df["√Årea Tem√°tica"] = df.apply(assign_area_extended_v2, axis=1)

    # Entrenar el modelo SVM si hay datos etiquetados
    df_labeled = df[df["√Årea Tem√°tica"] != "Otras"]
    if not df_labeled.empty:
        X = df_labeled["Title"].astype(str)
        y = df_labeled["√Årea Tem√°tica"]
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

        # Modelo SVM
        vectorizer = TfidfVectorizer(stop_words="english", ngram_range=(1,2), max_features=5000)
        model_svm = Pipeline([
            ("vectorizer", vectorizer),
            ("classifier", SVC(kernel="linear", probability=True))
        ])

        model_svm.fit(X_train, y_train)
        df_otros = df[df["√Årea Tem√°tica"] == "Otras"].copy()
        df_otros["√Årea Tem√°tica"] = model_svm.predict(df_otros["Title"].astype(str))
        df.update(df_otros)

    # Funci√≥n para generar nubes de palabras con traducci√≥n, stopwords eliminadas y lematizaci√≥n
    def generar_nubes_palabras(df):
        st.subheader("Nubes de Palabras por √Årea Tem√°tica")
        a√±os_disponibles = sorted(df["Year"].dropna().unique(), reverse=True)[:8]
        areas_interes = ["F√≠sica y Matem√°ticas", "Qu√≠mica", "Ingenier√≠a", "Medicina", "Biolog√≠a", "Humanidades"]

        stop_words = set(stopwords.words("english")) | set(stopwords.words("spanish")) | set(string.punctuation) | custom_stopwords
    
        def limpiar_texto(texto):
            texto = texto.lower()
            texto = re.sub(r"[\W_]+", " ", texto)  # Remover puntuaci√≥n y caracteres especiales
            palabras = texto.split()
            palabras_filtradas = [lemmatizer.lemmatize(word) for word in palabras if word not in stop_words and len(word) > 2]
            #palabras_traducidas = [translator.translate(word) for word in palabras_filtradas]
            palabras_traducidas = palabras_filtradas  # Mantener palabras originales sin traducir

            return " ".join(palabras_traducidas)

        for a√±o in a√±os_disponibles:
            df_a√±o = df[df["Year"] == a√±o]
            if df_a√±o.empty:
                continue

            st.subheader(f"A√±o {a√±o}")
            fig, axes = plt.subplots(2, 3, figsize=(18, 12))
            axes = axes.flatten()

            for i, area in enumerate(areas_interes):
                df_area = df_a√±o[df_a√±o["√Årea Tem√°tica"] == area]
                if not df_area.empty:
                    text = " ".join(df_area["Title"].dropna())
                    filtered_text = limpiar_texto(text)
                    wordcloud = WordCloud(width=800, height=400, background_color="white").generate(filtered_text)
                    axes[i].imshow(wordcloud, interpolation="bilinear")
                    axes[i].set_title(f"{area} ({a√±o})", fontsize=14)
                    axes[i].axis("off")
                else:
                    axes[i].axis("off")

            plt.tight_layout()
            st.pyplot(fig)

    # Generar nubes autom√°ticamente sin necesidad de bot√≥n
    generar_nubes_palabras(df)




###################################################################################
###################################################################################

    import streamlit as st
    import pandas as pd
    import matplotlib.pyplot as plt
    import seaborn as sns
    from wordcloud import WordCloud
    from collections import Counter
    from sklearn.feature_extraction.text import TfidfVectorizer
    import plotly.express as px
    import nltk
    from nltk.corpus import stopwords
    import string

    # Descargar stopwords si es la primera vez ejecutando el c√≥digo
    nltk.download("stopwords")
    nltk.download("wordnet")
    nltk.download("omw-1.4")

    # Inicializar lematizador y traductor
    lemmatizer = WordNetLemmatizer()
    #translator = GoogleTranslator(source='auto', target='english')  # Traducir todo a ingl√©s

    # Lista adicional de palabras comunes a excluir (convertidas a min√∫sculas para evitar problemas de coincidencia)
    custom_stopwords = {word.lower() for word in [
            "study", "method", "analysis", "model", "data", "results", "research", "approach", 
            "colima", "mexico", "asses", "assessment", "design", "mexican", "c√≥mo", "using", 
            "partial", "m√©xico", "effect", "comment", "based", "central", "evaluation", "employing", 
            "transformation", "application", "system", "approach", "n", "effects", "one", "two", "low", "high", "2021", "2020", "2019", "2022", "2018", "2017", "fast", "slow", "large", "small", ]}




        # Cargar el archivo CSV
        #file_path = "scopusUdeC con financiamiento 17 feb-2.csv"
        #df = pd.read_csv(file_path, encoding='latin1')

        # Diccionario extendido de palabras clave por √°rea tem√°tica
        #area_mapping_extended = {
        #    "F√≠sica y Matem√°ticas": ["Physical Review", "Mathematics", "Quantum", "Astrophysics", "Topology"],
        #    "Qu√≠mica": ["ChemEngineering", "Pharmaceuticals", "Chemical", "Biochemistry", "Catalysis"],
        #    "Ingenier√≠a": ["Engineering", "Robotics", "Technology", "Automation", "Materials Science"],
        #    "Medicina": ["Medicine", "Oncology", "Neurology", "Public Health", "Epidemiology"],
        #    "Biolog√≠a": ["Biology", "Microbiology", "Genomics", "Ecology", "Botany"],
        #    "Humanidades": ["Social Science", "History", "Philosophy", "Education", "Sociology"]
        #}

        # Funci√≥n para asignar un √°rea tem√°tica
        #def assign_area(row):
        #    source_title = str(row["Source title"])
        #    title = str(row["Title"])
        #    for area, keywords in area_mapping_extended.items():
        #        if any(keyword in source_title for keyword in keywords) or any(keyword in title for keyword in keywords):
        #            return area
        #    return "Otras"

        # Aplicar clasificaci√≥n inicial
        #df["√Årea Tem√°tica"] = df.apply(assign_area, axis=1)

        # Seleccionar el rango de a√±os en Streamlit
    a√±os_disponibles = sorted(df["Year"].dropna().unique(), reverse=True)
    a√±os_seleccionados = st.multiselect("Selecciona los a√±os a analizar", a√±os_disponibles, default=a√±os_disponibles[:8])
    df_filtrado = df[df["Year"].isin(a√±os_seleccionados)]

    # Definir stopwords en ingl√©s y espa√±ol
    #stop_words = set(stopwords.words("english") + stopwords.words("spanish") + list(string.punctuation))
    stop_words = set(stopwords.words("english")) | set(stopwords.words("spanish")) | set(string.punctuation) | custom_stopwords

    # Obtener los t√©rminos m√°s usados en cada √°rea tem√°tica
    def obtener_terminos(df, area):
        df_area = df[df["√Årea Tem√°tica"] == area]
        if df_area.empty:
            return None
    
        textos = " ".join(df_area["Title"].dropna()).lower()
        palabras = [word for word in textos.split() if word not in stop_words and len(word) > 3]
        conteo = Counter(palabras)
        terminos_comunes = conteo.most_common(10)
    
        autores_frecuentes = df_area["Authors"].value_counts().head(5).to_dict()
        return terminos_comunes, autores_frecuentes

    # Generar tablas por √°rea tem√°tica
    st.subheader("üîπ T√©rminos m√°s usados y autores destacados")
    for area in area_mapping_extended.keys():
        resultado = obtener_terminos(df_filtrado, area)
        if resultado:
            terminos, autores = resultado
            df_terminos = pd.DataFrame(terminos, columns=["T√©rmino", "Frecuencia"])
            st.write(f"**{area}**")
            st.dataframe(df_terminos)
            st.write("**Autores m√°s frecuentes en estos art√≠culos:**")
            for autor, conteo in autores.items():
                st.write(f"- {autor}: {conteo} art√≠culos")

    # Gr√°fico de pastel: proporci√≥n de art√≠culos por √°rea tem√°tica
    st.subheader("üìä Distribuci√≥n de art√≠culos por √°rea tem√°tica")
    df_areas = df_filtrado["√Årea Tem√°tica"].value_counts().reset_index()
    df_areas.columns = ["√Årea Tem√°tica", "Cantidad"]
    fig = px.pie(df_areas, names="√Årea Tem√°tica", values="Cantidad", title="Proporci√≥n de art√≠culos por √°rea tem√°tica")
    st.plotly_chart(fig)

#############################################################################################################################################

    import streamlit as st
    import pandas as pd
    import matplotlib.pyplot as plt
    from wordcloud import WordCloud
    import nltk
    from nltk.corpus import stopwords
    from nltk.stem import WordNetLemmatizer
    import string
    import re
    import plotly.express as px
    import plotly.graph_objects as go
    from collections import Counter

    # Descargar recursos de NLTK
    nltk.download("stopwords")
    nltk.download("wordnet")
    nltk.download("omw-1.4")

    # Inicializar lematizador
    lemmatizer = WordNetLemmatizer()

    # Lista adicional de palabras comunes a excluir (convertidas a min√∫sculas para evitar problemas de coincidencia)

    custom_stopwords = {word.lower() for word in [
            "study", "method", "analysis", "model", "data", "results", "research", "approach", 
            "colima", "mexico", "asses", "assessment", "design", "mexican", "c√≥mo", "using", 
            "partial", "m√©xico", "effect", "comment", "based", "central", "evaluation", "employing", 
            "transformation", "application", "system", "approach", "n", "effects", "one", "two", "low", "high", "2021", "2020", "2019", "2022", "2018", "2017", "fast", "slow", "large", "small", ]}


    # Configuraci√≥n de la aplicaci√≥n en Streamlit
    st.title("An√°lisis de √Åreas Tem√°ticas y Nubes de Palabras")

    # Diccionario extendido de palabras clave por √°rea tem√°tica
    area_mapping_extended = {
            "F√≠sica y Matem√°ticas": ["Physical Review", "Mathematics", "Quantum", "Astrophysics", "Topology"],
            "Qu√≠mica": ["ChemEngineering", "Pharmaceuticals", "Chemical", "Biochemistry", "Catalysis"],
            "Ingenier√≠a": ["Engineering", "Robotics", "Technology", "Automation", "Materials Science"],
            "Medicina": ["Medicine", "Oncology", "Neurology", "Public Health", "Epidemiology"],
            "Biolog√≠a": ["Biology", "Microbiology", "Genomics", "Ecology", "Botany"],
            "Humanidades": ["Social Science", "History", "Philosophy", "Education", "Sociology"]
    }

    # Funci√≥n para asignar un √°rea tem√°tica
    def assign_area_extended_v2(row):
        source_title = str(row["Source title"])
        title = str(row["Title"])
    
        for area, keywords in area_mapping_extended.items():
            if any(keyword in source_title for keyword in keywords) or any(keyword in title for keyword in keywords):
                return area
        return "Otras"

    # Aplicar clasificaci√≥n inicial
    df["√Årea Tem√°tica"] = df.apply(assign_area_extended_v2, axis=1)

    # Selecci√≥n del a√±o m√°s antiguo para visualizar
    a√±o_minimo = st.slider("Selecciona el a√±o m√°s antiguo para visualizar:", int(df["Year"].min()), int(df["Year"].max()), int(df["Year"].min()))

    # Funci√≥n para generar nubes de palabras con stopwords eliminadas y lematizaci√≥n
    def generar_nubes_palabras(df):
        st.subheader("Nubes de Palabras por √Årea Tem√°tica")
        a√±os_disponibles = sorted(df["Year"].dropna().unique())
        a√±os_disponibles = [a for a in a√±os_disponibles if a >= a√±o_minimo]
        areas_interes = list(area_mapping_extended.keys())

        stop_words = set(stopwords.words("english")) | set(stopwords.words("spanish")) | set(string.punctuation) | custom_stopwords
    
        def limpiar_texto(texto):
            texto = texto.lower()
            texto = re.sub(r"[\W_]+", " ", texto)  # Remover puntuaci√≥n y caracteres especiales
            palabras = texto.split()
            palabras_filtradas = [lemmatizer.lemmatize(word) for word in palabras if word not in stop_words and len(word) > 2]
            return " ".join(palabras_filtradas)

        global word_frequencies
        word_frequencies = {area: Counter() for area in areas_interes}

        for a√±o in a√±os_disponibles:
            df_a√±o = df[df["Year"] == a√±o]
            if df_a√±o.empty:
                continue

            #st.subheader(f"A√±o {a√±o}")
            for area in areas_interes:
                df_area = df_a√±o[df_a√±o["√Årea Tem√°tica"] == area]
                if not df_area.empty:
                    text = " ".join(df_area["Title"].dropna())
                    filtered_text = limpiar_texto(text)
                
                    # Acumular las frecuencias de palabras
                    word_counts = Counter(filtered_text.split())
                    word_frequencies[area] += word_counts

    # Generar nubes autom√°ticamente
    generar_nubes_palabras(df)

    # Generar gr√°fica de barras animada separada por √°rea
    def generar_animacion_palabras(word_frequencies):
        st.subheader("üìä Evoluci√≥n del Uso de Palabras Clave en √Åreas Tem√°ticas")
    
        for area, counter in word_frequencies.items():
            data = []
            for palabra, frecuencia in counter.most_common(30):
                 data.append({"Palabra": palabra, "Frecuencia": frecuencia})
        
            df_animacion = pd.DataFrame(data)
            if not df_animacion.empty:
                fig = px.bar(
                    df_animacion,
                    x="Frecuencia",
                    y="Palabra",
                    orientation="h",
                    title=f"Top 30 Palabras M√°s Usadas en {area}",
                    labels={"Frecuencia": "Frecuencia de Uso", "Palabra": "Palabras Clave"},
                    template="plotly_white"
                )
                fig.update_layout(height=900, xaxis=dict(range=[0, df_animacion["Frecuencia"].max() * 1.1]))
                st.plotly_chart(fig)

    # Generar la animaci√≥n por √°rea
    generar_animacion_palabras(word_frequencies)


    #import streamlit as st
    #import pandas as pd
    #import matplotlib.pyplot as plt
    #import seaborn as sns
    #from collections import Counter
    #import nltk    
    #from nltk.corpus import stopwords
    #from nltk.stem import WordNetLemmatizer
    #import string
    #import re
    #import numpy as np
    #from sklearn.metrics.pairwise import cosine_similarity
    #from sentence_transformers import SentenceTransformer
    #from sklearn.cluster import AgglomerativeClustering

    ## Descargar recursos de NLTK
    #nltk.download("stopwords")
    #nltk.download("wordnet")
    #nltk.download("omw-1.4")

    ## Filtrar por √°rea tem√°tica
    #df_fisica = df[df["√Årea Tem√°tica"] == "F√≠sica y Matem√°ticas"].copy()
    #df_fisica = df_fisica[df_fisica["Year"].notna()]
    #df_fisica["Year"] = df_fisica["Year"].astype(int)

    ## Stopwords y lematizador
    #custom_stopwords = {word.lower() for word in [
    #"study", "method", "analysis", "model", "data", "results", "research", "approach", 
    #"colima", "mexico", "asses", "assessment", "design", "mexican", "c√≥mo", "using", 
    #"partial", "m√©xico", "effect", "comment", "based", "central", "evaluation", "employing", 
    #"transformation", "application", "system", "approach", "n", "effects", "one", "two", "low", 
    #"high", "2021", "2020", "2019", "2022", "2018", "2017", "fast", "slow", "large", "small"
    #]}
    #stop_words = set(stopwords.words("english")) | set(stopwords.words("spanish")) | set(string.punctuation) | custom_stopwords
    #lemmatizer = WordNetLemmatizer()

    #def limpiar_texto(texto):
    #    texto = texto.lower()
    #    texto = re.sub(r"[\W_]+", " ", texto)
    #    palabras = texto.split()
    #    palabras_filtradas = []

    #    for word in palabras:
    #        if word in stop_words:
    #            continue
    #        if len(word) <= 2:
    #            continue
    #        if word.isnumeric() or any(char.isdigit() for char in word):
    #            continue
    #        lemma = lemmatizer.lemmatize(word)
    #        palabras_filtradas.append(lemma)

    #    return palabras_filtradas

    ## Extraer subtemas principales por a√±o
    #a√±os_disponibles = sorted(df_fisica["Year"].unique())
    #subtemas_por_a√±o = {}

    #for a√±o in a√±os_disponibles:
    #    titulos = df_fisica[df_fisica["Year"] == a√±o]["Title"].dropna()
    #    palabras = []
    #    for titulo in titulos:
    #        palabras.extend(limpiar_texto(str(titulo)))
    #    conteo = Counter(palabras)
    #    subtemas_comunes = [palabra for palabra, _ in conteo.most_common(30)]
    #    subtemas_por_a√±o[a√±o] = subtemas_comunes

    ## Agrupar subtemas similares usando embeddings
    #modelo = SentenceTransformer('all-MiniLM-L6-v2')
    #todos_los_subtemas = sorted(set(p for lista in subtemas_por_a√±o.values() for p in lista))
    #embeddings = modelo.encode(todos_los_subtemas)
    #dist_matrix = 1 - cosine_similarity(embeddings)

    ## Clustering jer√°rquico con threshold m√°s amplio (menos grupos)
    #clustering = AgglomerativeClustering(n_clusters=None, distance_threshold=0.6, metric='precomputed', linkage='average')
    #labels = clustering.fit_predict(dist_matrix)

    ## Mapear subtemas a su cluster
    #grupo_por_subtema = {subtema: f"Grupo {label}" for subtema, label in zip(todos_los_subtemas, labels)}

    ## Crear matriz de presencia por subtema (no grupo) para identificar los m√°s persistentes
    #matriz_subtemas = pd.DataFrame(0, index=todos_los_subtemas, columns=a√±os_disponibles)
    #for a√±o in a√±os_disponibles:
    #    for subtema in subtemas_por_a√±o[a√±o]:
    #        if subtema in matriz_subtemas.index:
    #            matriz_subtemas.loc[subtema, a√±o] = 1

    ## Calcular cu√°ntos a√±os ha estado presente cada subtema
    #matriz_subtemas["A√±os Activo"] = matriz_subtemas.sum(axis=1)
    #subtemas_mas_constantes = matriz_subtemas.sort_values("A√±os Activo", ascending=False).head(20)

    ## Visualizaci√≥n
    #st.title("üåø Subtemas m√°s persistentes en F√≠sica y Matem√°ticas")
    #st.markdown("Estos son los subtemas que m√°s veces han aparecido a lo largo de los a√±os en los t√≠tulos de art√≠culos.")

    #fig, ax = plt.subplots(figsize=(10, 8))
    #sns.heatmap(subtemas_mas_constantes.drop(columns="A√±os Activo"), cmap="YlGnBu", linewidths=0.5, linecolor='gray', cbar=False, ax=ax)
    #ax.set_title("Subtemas con mayor presencia en el tiempo")
    #ax.set_xlabel("A√±o")
    #ax.set_ylabel("Subtema")
    #st.pyplot(fig)

    ## Mostrar tambi√©n la tabla de resumen
    #st.subheader("üìä A√±os en los que ha estado presente cada subtema")
    #st.dataframe(subtemas_mas_constantes["A√±os Activo"].to_frame())
    
    import streamlit as st
    import pandas as pd
    import matplotlib.pyplot as plt
    import seaborn as sns
    from collections import Counter, defaultdict
    import nltk    
    from nltk.corpus import stopwords
    from nltk.stem import WordNetLemmatizer
    import string
    import re
    import numpy as np
    from sklearn.metrics.pairwise import cosine_similarity
    from sentence_transformers import SentenceTransformer
    from sklearn.cluster import AgglomerativeClustering

    # Descargar recursos de NLTK
    nltk.download("stopwords")
    nltk.download("wordnet")
    nltk.download("omw-1.4")

    # Cargar los datos
    #file_path = "scopusUdeC con financiamiento 17 feb-2.csv"
    #df = pd.read_csv(file_path, encoding="utf-8")
    #df = df[df["Year"].notna()]
    #df["Year"] = df["Year"].astype(int)

    # Diccionario de √°reas tem√°ticas
    area_mapping_extended = {
    "F√≠sica y Matem√°ticas": ["Physical Review", "Mathematics", "Quantum", "Astrophysics", "Topology"],
    "Qu√≠mica": ["ChemEngineering", "Pharmaceuticals", "Chemical", "Biochemistry", "Catalysis"],
    "Ingenier√≠a": ["Engineering", "Robotics", "Technology", "Automation", "Materials Science"],
    "Medicina": ["Medicine", "Oncology", "Neurology", "Public Health", "Epidemiology"],
    "Biolog√≠a": ["Biology", "Microbiology", "Genomics", "Ecology", "Botany"],
    "Humanidades": ["Social Science", "History", "Philosophy", "Education", "Sociology"]
    }

    # Stopwords y lematizador
    custom_stopwords = {word.lower() for word in [
    "study", "method", "analysis", "model", "data", "results", "research", "approach", 
    "colima", "mexico", "asses", "assessment", "design", "mexican", "c√≥mo", "using", 
    "partial", "m√©xico", "effect", "comment", "based", "central", "evaluation", "employing", 
    "transformation", "application", "system", "approach", "n", "effects", "one", "two", "low", 
    "high", "2021", "2020", "2019", "2022", "2018", "2017", "fast", "slow", "large", "small"
    ]}
    #stop_words = set(stopwords.words("english")) | set(stopwords.words("spanish")) | set(string.punctuation) | custom_stopwords
    lemmatizer = WordNetLemmatizer()

    def limpiar_texto(texto):
        texto = texto.lower()
        texto = re.sub(r"[\W_]+", " ", texto)
        palabras = texto.split()
        palabras_filtradas = []

        for word in palabras:
            if word in stop_words:
                continue
            if len(word) <= 2:
                continue
            if word.isnumeric() or any(char.isdigit() for char in word):
                continue
            lemma = lemmatizer.lemmatize(word)
            palabras_filtradas.append(lemma)

        return palabras_filtradas

    # Interfaz Streamlit
    st.title("üìä Subtemas Persistentes por √Årea Tem√°tica")
    area_seleccionada = st.selectbox("Selecciona un √°rea tem√°tica:", list(area_mapping_extended.keys()))

    df_area = df[df["√Årea Tem√°tica"] == area_seleccionada].copy()
    if df_area.empty:
        st.warning("No hay datos disponibles para esta √°rea tem√°tica.")
    else:
        a√±os_disponibles = sorted(df_area["Year"].unique())
        subtemas_por_a√±o = {}

        for a√±o in a√±os_disponibles:
            titulos = df_area[df_area["Year"] == a√±o]["Title"].dropna()
            palabras = []
            for titulo in titulos:
                palabras.extend(limpiar_texto(str(titulo)))
            conteo = Counter(palabras)
            subtemas_comunes = [palabra for palabra, _ in conteo.most_common(30)]
            subtemas_por_a√±o[a√±o] = subtemas_comunes

        # Agrupar subtemas similares usando embeddings
        modelo = SentenceTransformer('all-MiniLM-L6-v2')
        todos_los_subtemas = sorted(set(p for lista in subtemas_por_a√±o.values() for p in lista))
        if todos_los_subtemas:
            embeddings = modelo.encode(todos_los_subtemas)
            dist_matrix = 1 - cosine_similarity(embeddings)

            clustering = AgglomerativeClustering(n_clusters=None, distance_threshold=0.6, metric='precomputed', linkage='average')
            labels = clustering.fit_predict(dist_matrix)

            grupo_por_subtema = {subtema: f"Grupo {label}" for subtema, label in zip(todos_los_subtemas, labels)}

            # Crear matriz de presencia por subtema
            matriz_subtemas = pd.DataFrame(0, index=todos_los_subtemas, columns=a√±os_disponibles)
            for a√±o in a√±os_disponibles:
                for subtema in subtemas_por_a√±o[a√±o]:
                    if subtema in matriz_subtemas.index:
                        matriz_subtemas.loc[subtema, a√±o] = 1

            matriz_subtemas["A√±os Activo"] = matriz_subtemas.sum(axis=1)
            subtemas_mas_constantes = matriz_subtemas.sort_values("A√±os Activo", ascending=False).head(20)

            st.markdown(f"### üåø Subtemas m√°s persistentes en {area_seleccionada}")
            fig, ax = plt.subplots(figsize=(10, 8))
            sns.heatmap(subtemas_mas_constantes.drop(columns="A√±os Activo"), cmap="YlGnBu", linewidths=0.5, linecolor='gray', cbar=False, ax=ax)
            ax.set_title("Subtemas con mayor presencia en el tiempo")
            ax.set_xlabel("A√±o")
            ax.set_ylabel("Subtema")
            st.pyplot(fig)

            st.subheader("üìä A√±os en los que ha estado presente cada subtema")
            st.dataframe(subtemas_mas_constantes["A√±os Activo"].to_frame())
        else:
            st.info("No se encontraron subtemas frecuentes para esta √°rea.")


##########################################################################################################################

#    import streamlit as st
#    import pandas as pd
#    import networkx as nx
#    import plotly.graph_objects as go
#    import string
#    import re
#    from collections import Counter

## Cargar datos
##file_path = "scopusUdeC con financiamiento 17 feb-2.csv"
##df = pd.read_csv(file_path, encoding="utf-8")
##df = df[df["Year"].notna()]
##df["Year"] = df["Year"].astype(int)

## Stopwords locales
##stop_words_en = {"the", "and", "for", "with", "that", "from", "this", "using", "into", "been",
##                 "their", "between", "about", "than", "also", "have", "which", "such", "more", "most"}
##stop_words_es = {"los", "las", "que", "con", "una", "por", "para", "del", "m√°s", "como", "entre",
##                 "sus", "este", "esta", "estos", "estas", "tambi√©n", "pero", "sobre"}
##custom_stopwords = {"study", "method", "analysis", "data", "results", "research", "approach", 
##                    "colima", "mexico", "m√©xico", "c√≥mo"}
##stop_words = stop_words_en | stop_words_es | custom_stopwords | set(string.punctuation)

#    def limpiar_texto(texto):
#        texto = texto.lower()
#        texto = re.sub(r"[\W_]+", " ", texto)
#        palabras = texto.split()
#        return [
#            word for word in palabras
#            if word not in stop_words and len(word) > 2 and not re.match(r"^(19|20)\d{2}$", word)
#    ]

#    # Interfaz
#    areas_disponibles = sorted(df["√Årea Tem√°tica"].dropna().unique())
#area_seleccionada = st.selectbox("Selecciona un √°rea tem√°tica:", areas_disponibles)
#    #df_area = df[df["√Årea Tem√°tica"] == area_seleccionada]
#    a√±os_disponibles = sorted(df_area["Year"].unique())

#    # Crear grafo
#    G = nx.DiGraph()
#    G.add_node(area_seleccionada)

#    subtemas_por_a√±o = {}
#    for a√±o in a√±os_disponibles:
#        nodo_a√±o = f"A√±o {a√±o}"
#        G.add_node(nodo_a√±o)
#        G.add_edge(area_seleccionada, nodo_a√±o)
    
#        titulos = df_area[df_area["Year"] == a√±o]["Title"].dropna()
#        palabras = []
#        for titulo in titulos:
#            palabras.extend(limpiar_texto(str(titulo)))
#        conteo = Counter(palabras)
#        subtemas = [palabra for palabra, _ in conteo.most_common(5)]
#        subtemas_por_a√±o[a√±o] = subtemas
    
#        for subtema in subtemas:
#            if not G.has_node(subtema):
#                G.add_node(subtema)
#            G.add_edge(nodo_a√±o, subtema)

#    # Selector m√∫ltiple
#    a√±os_seleccionados = st.multiselect("Selecciona uno o m√°s a√±os para mostrar sus subtemas:", a√±os_disponibles, default=[a√±os_disponibles[-1]])

#    # Organizar layout en capas tipo √°rbol
#    nodo_raiz = area_seleccionada
#    nodos_de_a√±os = [f"A√±o {a}" for a in a√±os_disponibles]
#    nodos_de_subtemas = [n for n in G.nodes() if n not in nodos_de_a√±os and n != nodo_raiz]
#    pos = nx.shell_layout(G, nlist=[[nodo_raiz], nodos_de_a√±os, nodos_de_subtemas])

#    # Edges
#    edge_x, edge_y = [], []
#    for edge in G.edges():
#        x0, y0 = pos[edge[0]]
#        x1, y1 = pos[edge[1]]
#        edge_x.extend([x0, x1, None])
#        edge_y.extend([y0, y1, None])

#    edge_trace = go.Scatter(x=edge_x, y=edge_y, line=dict(width=0.5, color='#888'), hoverinfo='none', mode='lines')

#    # Nodes
#    node_x, node_y, node_text, node_color, node_opacity = [], [], [], [], []
#    nodos_activados = {area_seleccionada} | {f"A√±o {a}" for a in a√±os_seleccionados}
#    for a in a√±os_seleccionados:
#        nodos_activados |= set(subtemas_por_a√±o.get(a, []))

#    for node in G.nodes():
#        x, y = pos[node]
#        node_x.append(x)
#        node_y.append(y)
#        node_text.append(node)
#        if node in nodos_activados:
#            node_color.append("green")
#            node_opacity.append(1.0)
#        else:
#            node_color.append("lightgray")
#            node_opacity.append(0.2)

#    node_trace = go.Scatter(
#        x=node_x, y=node_y,
#        mode='markers+text',
#        text=node_text,
#        textposition="top center",
#        hoverinfo='text',
#        marker=dict(
#            showscale=False,
#            color=node_color,
#            opacity=node_opacity,
#            size=14,
#            line_width=2
#        )
#    )

#    fig = go.Figure(data=[edge_trace, node_trace],
#                layout=go.Layout(
#                    title=dict(
#                        text=f"üå± Subtemas de {area_seleccionada} en {', '.join(map(str, a√±os_seleccionados))}",
#                        font=dict(size=16)
#                    ),
#                    showlegend=False,
#                    hovermode='closest',
#                    margin=dict(b=20, l=5, r=5, t=40),
#                    xaxis=dict(showgrid=False, zeroline=False),
#                    yaxis=dict(showgrid=False, zeroline=False)
#                ))

#    st.plotly_chart(fig, use_container_width=True)


############################################################################################################################

    import streamlit as st
    import pandas as pd
    import networkx as nx
    import plotly.graph_objects as go
    import nltk
    from nltk.corpus import stopwords
    import string
    import re
    from collections import Counter

    nltk.download("stopwords")

    # Cargar los datos
    #file_path = "scopusUdeC con financiamiento 17 feb-2.csv"
    #df = pd.read_csv(file_path, encoding="utf-8")
    #df = df[df["Year"].notna()]
    #df["Year"] = df["Year"].astype(int)

    # Selecci√≥n de √°rea tem√°tica
    #areas_disponibles = sorted(df["√Årea Tem√°tica"].dropna().unique())
    #area_seleccionada = st.selectbox("Selecciona un √°rea tem√°tica:", areas_disponibles)
    #df_area = df[df["√Årea Tem√°tica"] == area_seleccionada]

# Stopwords
    #custom_stopwords = {word.lower() for word in [
    #    "study", "method", "analysis", "model", "data", "results", "research", "approach", 
    #    "colima", "mexico", "asses", "assessment", "design", "mexican", "c√≥mo", "using", 
    #    "partial", "m√©xico", "effect", "comment", "based", "central", "evaluation", "employing", 
    #    "transformation", "application", "system", "approach", "n", "effects", "one", "two", 
    #    "low", "high", "2021", "2020", "2019", "2022", "2018", "2017", "fast", "slow", 
    #    "large", "small"
    #]}
    #stop_words = set(stopwords.words("english")) | set(stopwords.words("spanish")) | set(string.punctuation) | custom_stopwords

    def limpiar_texto(texto):
        texto = texto.lower()
        texto = re.sub(r"[\W_]+", " ", texto)
        palabras = texto.split()
        return [
            word for word in palabras
            if word not in stop_words and len(word) > 2 and not re.match(r"^(19|20)\d{2}$", word)
        ]

    # Subtemas y grafo
    a√±os_disponibles = sorted(df_area["Year"].unique())
    a√±os_seleccionados = st.multiselect("Selecciona uno o m√°s a√±os:", a√±os_disponibles, default=a√±os_disponibles[-5:])

    G = nx.DiGraph()
    G.add_node(area_seleccionada)
    subtemas_por_a√±o = {}
    frecuencia_total = Counter()

    for a√±o in a√±os_disponibles:
        nodo_a√±o = f"A√±o {a√±o}"
        G.add_node(nodo_a√±o)
        G.add_edge(area_seleccionada, nodo_a√±o)

        titulos = df_area[df_area["Year"] == a√±o]["Title"].dropna()
        palabras = []
        for titulo in titulos:
            palabras.extend(limpiar_texto(str(titulo)))
        conteo = Counter(palabras)
        subtemas = [palabra for palabra, _ in conteo.most_common(10)]
        subtemas_por_a√±o[a√±o] = subtemas
        frecuencia_total.update(subtemas)

        for subtema in subtemas:
            if not G.has_node(subtema):
                G.add_node(subtema)
            G.add_edge(nodo_a√±o, subtema)

    # Subtemas en m√∫ltiples a√±os
    subtemas_en_varios_a√±os = Counter()
    for a√±o in a√±os_seleccionados:
        for subtema in subtemas_por_a√±o[a√±o]:
            subtemas_en_varios_a√±os[subtema] += 1

    # Layout (shell para estilo √°rbol)
    nodo_raiz = area_seleccionada
    nodos_de_a√±os = [f"A√±o {a}" for a in a√±os_disponibles]
    nodos_de_subtemas = [n for n in G.nodes() if n not in nodos_de_a√±os and n != nodo_raiz]
    pos = nx.shell_layout(G, nlist=[[nodo_raiz], nodos_de_a√±os, nodos_de_subtemas])

    # Edges
    edge_x, edge_y = [], []
    for edge in G.edges():
        x0, y0 = pos[edge[0]]
        x1, y1 = pos[edge[1]]
        edge_x.extend([x0, x1, None])
        edge_y.extend([y0, y1, None])

    edge_trace = go.Scatter(
        x=edge_x, y=edge_y,
        line=dict(width=0.5, color='#888'),
        hoverinfo='none',
        mode='lines'
    )

    # Nodes
    node_x, node_y, node_text, node_color, node_size = [], [], [], [], []

    # Aseg√∫rate de tener estas listas definidas:
    node_opacity = []
    node_sizes = []

    for node in G.nodes():
        frecuencia = frecuencia_total.get(node, 1)
        x, y = pos[node]
        node_x.append(x)
        node_y.append(y)
        node_text.append(f"{node} ({frecuencia} veces)")

        # Color y opacidad seg√∫n si est√° seleccionado
        if node in nodos_activados:
            if frecuencia > 1 and node not in nodos_a√±os and node != area_seleccionada:
                node_colors.append("blue")  # Subtema compartido
            else:
                node_colors.append("green")
            node_opacity.append(1.0)
        else:
            node_colors.append("lightgray")
            node_opacity.append(0.2)

        # Tama√±o proporcional a frecuencia
        node_sizes.append(10 + 4 * frecuencia)

    # Grafo sin texto visible, solo en hover
    node_trace = go.Scatter(
        x=node_x,
        y=node_y,
        mode='markers',
        text=node_text,  # Tooltip
        hoverinfo='text',
        marker=dict(
            size=node_sizes,
            color=node_colors,
            opacity=node_opacity,
            line_width=1.5
        )
    )

    fig = go.Figure(data=[edge_trace, node_trace],
        layout=go.Layout(
            title=dict(
                text=f"üå± Subtemas de {area_seleccionada} en {', '.join(map(str, a√±os_seleccionados))}",
                font=dict(size=16)
            ),
            showlegend=False,
            hovermode='closest',
            margin=dict(b=20, l=5, r=5, t=60),
            xaxis=dict(showgrid=False, zeroline=False),
            yaxis=dict(showgrid=False, zeroline=False)
        )
    )



    st.plotly_chart(fig, use_container_width=True)






    



##############################################################################################################################3
elif pagina == "An√°lisis por autor":
    # Secci√≥n Markdown explicativa
    st.markdown("""
    ## An√°lisis de Datos por Autor

    El siguiente c√≥digo muestra algunos datos por autor. Al correrlo:

    1. **Ingrese el apellido del autor**. Se desplegar√°n todos los autores que compartan apellido. En muchos casos, se mostrar√° m√°s de un nombre por ID debido a que en ocasiones el nombre con el que se firma el art√≠culo var√≠a.
    2. **Ingrese el ID del autor** para ver los datos asociados a ese ID.

    El tipo de informaci√≥n que se desplegar√° ser√°:
    - **Autores asociados con el ID**
    - **Total de citas asociadas**
    - **Total de art√≠culos en los que participa el ID**
    - **A√±o m√°s antiguo de publicaci√≥n**
    - **A√±o m√°s reciente de publicaci√≥n**

    Adem√°s, se generar√°n las siguientes gr√°ficas:
    - **Total de publicaciones vs. tiempo**
    - **Total de citas vs. tiempo**
    - **Gr√°fico de barras con las principales editoriales en las que publica el autor**
    """)



    import streamlit as st
    import pandas as pd
    import re
    import matplotlib.pyplot as plt

    @st.cache_data
    def load_data(file):
        df = pd.read_csv(file, encoding='utf-8')
        return df


    # Funci√≥n para obtener autores por apellido
    @st.cache_data
    def get_author_options(df, author_last_name):
        if "Authors" not in df.columns or "Author(s) ID" not in df.columns:
            st.error("No se encontraron las columnas necesarias en el archivo.")
            return {}

        author_dict = {}
        for _, row in df.dropna(subset=["Authors", "Author(s) ID"]).iterrows():
            authors = row["Authors"].split(";")
            ids = str(row["Author(s) ID"]).split(";")
            for author, author_id in zip(authors, ids):
                author = author.strip()
                if author_last_name.lower() in author.lower():
                    author_dict.setdefault(author_id.strip(), []).append(author)

        return author_dict

    # Funci√≥n para obtener nombres de autores por ID
    @st.cache_data
    def get_authors_by_id(df, selected_id):
        if "Authors" not in df.columns or "Author(s) ID" not in df.columns:
            st.error("No se encontraron las columnas necesarias en el archivo.")
            return []

        matching_authors = set()
        for _, row in df.dropna(subset=["Authors", "Author(s) ID"]).iterrows():
            authors = row["Authors"].split(";")
            ids = str(row["Author(s) ID"]).split(";")
            for author, author_id in zip(authors, ids):
                if author_id.strip() == selected_id:
                    matching_authors.add(author.strip())

        return list(matching_authors)

    # Funci√≥n para obtener estad√≠sticas del autor
    @st.cache_data
    def get_author_stats(df, selected_id):
        if "Author(s) ID" not in df.columns:
            st.error("No se encontraron las columnas necesarias en el archivo.")
            return {}

        df_filtered = df[df["Author(s) ID"].str.contains(selected_id, na=False, case=False)]
    
        total_citations = df_filtered["Cited by"].fillna(0).astype(int).sum() if "Cited by" in df.columns else 0
        total_articles = df_filtered.shape[0]
    
        min_year, max_year, year_counts = None, None, None
        if "Year" in df.columns:
            years = df_filtered["Year"].dropna().astype(int)
            if not years.empty:
                min_year, max_year = years.min(), years.max()
                year_counts = years.value_counts().sort_index()

        publisher_info = pd.DataFrame()
        if "Publisher" in df.columns and "Cited by" in df.columns:
            publisher_info = df_filtered.groupby("Publisher").agg(
                num_articles=("Title", "count"),
                total_citations=("Cited by", "sum")
            ).reset_index()

        return {
            "total_citations": total_citations,
            "total_articles": total_articles,
            "min_year": min_year,
            "max_year": max_year,
            "year_counts": year_counts,
            "publisher_info": publisher_info
        }

    # Funci√≥n para graficar las publicaciones por a√±o
    def plot_publications(year_counts, selected_id):
        if year_counts is None or year_counts.empty:
            st.warning("No se encontraron a√±os de publicaci√≥n para este autor.")
            return
    
        fig, ax = plt.subplots(figsize=(10, 5))
        year_counts.plot(kind='bar', color='blue', ax=ax)
        ax.set_xlabel("A√±o de publicaci√≥n")
        ax.set_ylabel("Numero de publicaciones")
        ax.set_title(f"Publicaciones por a√±o - ID {selected_id}")
        ax.set_xticklabels(year_counts.index, rotation=45)
        st.pyplot(fig)

    #@st.cache_data
    uploaded_file = st.file_uploader("Suba un archivo CSV", type=["csv"])
    file_path = "scopusUdeC con financiamiento 17 feb-2.csv"
    # Funci√≥n para cargar y almacenar el DataFrame en cach√©
    #@st.cache_data
    #def load_data(file):
    #    return pd.read_csv(file, encoding='utf-8')


    #def get_total_citations(file_path, selected_id):
    #    #df = pd.read_csv(file_path, encoding='utf-8')
    #    df = load_data(uploaded_file) 
    #    if "Author(s) ID" not in df.columns or "Cited by" not in df.columns:
    #        print("No se encontraron las columnas necesarias en el archivo.")
    #        return 0

    #    df_filtered = df[df["Author(s) ID"].str.contains(selected_id, na=False, case=False)]
    #    total_citations = df_filtered["Cited by"].fillna(0).astype(int).sum()

    #    return total_citations

    def get_total_citations(df, selected_id):
        """Calcula el total de citas asociadas a un ID de autor en el DataFrame."""
        if "Author(s) ID" not in df.columns or "Cited by" not in df.columns:
            st.error("‚ùå No se encontraron las columnas necesarias en el archivo.")
            return 0

        df_filtered = df[df["Author(s) ID"].str.contains(selected_id, na=False, case=False)]
        total_citations = df_filtered["Cited by"].fillna(0).astype(int).sum()

        return total_citations

    
    #def get_total_articles(file_path, selected_id):
    #    df = pd.read_csv(file_path, encoding='utf-8')
    #    if "Author(s) ID" not in df.columns:
    #        print("No se encontraron las columnas necesarias en el archivo.")
    #        return 0

   #     df_filtered = df[df["Author(s) ID"].str.contains(selected_id, na=False, case=False)]
   #     total_articles = df_filtered.shape[0]

   #     return total_articles

    def get_total_articles(df, selected_id):
        """Calcula el total de art√≠culos en los que participa un ID de autor."""
        if "Author(s) ID" not in df.columns:
            st.error("‚ùå No se encontr√≥ la columna 'Author(s) ID' en el archivo.")
            return 0

        df_filtered = df[df["Author(s) ID"].str.contains(selected_id, na=False, case=False)]
        total_articles = df_filtered.shape[0]

        return total_articles



    #def get_publication_years(file_path, selected_id):
    #    df = pd.read_csv(file_path, encoding='utf-8')
    #    if "Author(s) ID" not in df.columns or "Year" not in df.columns:
    #        print("No se encontraron las columnas necesarias en el archivo.")
    #        return None, None, None, None

    #    df_filtered = df[df["Author(s) ID"].str.contains(selected_id, na=False, case=False)]
    #    years = df_filtered["Year"].dropna().astype(int)
    #    if years.empty:
    #        return None, None, None, None

    #    min_year = years.min()
    #    max_year = years.max()
    #    year_counts = years.value_counts().sort_index()

    #    # Total de citas por a√±o
    #    citations_per_year = df_filtered.groupby("Year")["Cited by"].sum().fillna(0).astype(int)

    #    return min_year, max_year, year_counts, citations_per_year

    def get_publication_years(df, selected_id):
        """Obtiene el rango de a√±os de publicaci√≥n y el n√∫mero de publicaciones/citas por a√±o de un autor."""
        df = load_data(uploaded_file)
        if "Author(s) ID" not in df.columns or "Year" not in df.columns:
            st.error("‚ùå No se encontraron las columnas necesarias en el archivo.")
            return None, None, None, None

        df_filtered = df[df["Author(s) ID"].str.contains(selected_id, na=False, case=False)]
        years = df_filtered["Year"].dropna().astype(int)

        if years.empty:
            return None, None, None, None

        min_year = years.min()
        max_year = years.max()
        year_counts = years.value_counts().sort_index()

        # Total de citas por a√±o
        if "Cited by" in df_filtered.columns:
            citations_per_year = df_filtered.groupby("Year")["Cited by"].sum().fillna(0).astype(int)
        else:
            citations_per_year = None

        return min_year, max_year, year_counts, citations_per_year


#    def get_publisher_info(file_path, selected_id):
#        df = pd.read_csv(file_path, encoding='utf-8')
#        if "Author(s) ID" not in df.columns or "Publisher" not in df.columns or "Cited by" not in df.columns:
#            print("No se encontraron las columnas necesarias en el archivo.")
#            return None

#        df_filtered = df[df["Author(s) ID"].str.contains(selected_id, na=False, case=False)]
#        publisher_stats = df_filtered.groupby("Publisher").agg(
#            num_articles=("Title", "count"),
#            total_citations=("Cited by", "sum")
#        ).reset_index()

#        return publisher_stats

    def get_publisher_info(df, selected_id):
        """Obtiene informaci√≥n sobre las editoriales donde ha publicado un autor."""
        required_columns = ["Author(s) ID", "Publisher", "Cited by", "Title"]

        # Verificar que todas las columnas necesarias existen
        missing_columns = [col for col in required_columns if col not in df.columns]
        if missing_columns:
            st.error(f"‚ùå No se encontraron las columnas necesarias en el archivo: {', '.join(missing_columns)}")
            return None

        # Filtrar los art√≠culos en los que ha participado el autor
        df_filtered = df[df["Author(s) ID"].str.contains(selected_id, na=False, case=False)]

        # Agrupar por editorial y calcular n√∫mero de art√≠culos y citas
        publisher_stats = df_filtered.groupby("Publisher").agg(
            num_articles=("Title", "count"),
            total_citations=("Cited by", "sum")
        ).reset_index()

        return publisher_stats

    def get_top_cited_articles(df, selected_id, top_n=10):
        """
        Obtiene los art√≠culos m√°s citados de un autor, incluyendo t√≠tulo, n√∫mero de citas, a√±o y editorial.

        Par√°metros:
        - df: DataFrame con los datos de publicaciones.
        - selected_id: ID del autor.
        - top_n: N√∫mero de art√≠culos a mostrar (por defecto, 10).

        Retorna:
        - DataFrame con los art√≠culos m√°s citados del autor.
        """

        required_columns = ["Author(s) ID", "Title", "Cited by", "Year", "Source title", "Publisher"]

        # Verificar que todas las columnas necesarias existen
        missing_columns = [col for col in required_columns if col not in df.columns]
        if missing_columns:
            st.error(f"‚ùå No se encontraron las columnas necesarias en el archivo: {', '.join(missing_columns)}")
            return None

        # Filtrar los art√≠culos en los que ha participado el autor
        df_filtered = df[df["Author(s) ID"].str.contains(selected_id, na=False, case=False)]

        # Asegurar que "Cited by" y "Year" sean valores num√©ricos
        df_filtered["Cited by"] = pd.to_numeric(df_filtered["Cited by"], errors="coerce").fillna(0).astype(int)
        #df_filtered["Year"] = pd.to_numeric(df_filtered["Year"], errors="coerce")
        df_filtered["Year"] = pd.to_numeric(df_filtered["Year"], errors="coerce").fillna(0).astype(int)

        # Seleccionar las columnas necesarias
        top_articles = df_filtered[["Title", "Cited by", "Year", "Source title", "Publisher"]]

        # Ordenar por n√∫mero de citas en orden descendente
        top_articles = top_articles.sort_values(by="Cited by", ascending=False).head(top_n)

        return top_articles




    def plot_publications(year_counts, selected_id):
        if year_counts is not None and not year_counts.empty:
            fig, ax = plt.subplots(figsize=(10, 5))
            year_counts.plot(kind='bar', color='blue', ax=ax)
            ax.set_xlabel("A√±o de publicaci√≥n")
            ax.set_ylabel("N√∫mero de publicaciones")
            ax.set_title(f"Publicaciones por a√±o - ID {selected_id}")
            ax.set_xticklabels(year_counts.index, rotation=45)
            st.pyplot(fig)

    def plot_citations_per_year(citations_per_year, selected_id):
        if citations_per_year is not None and not citations_per_year.empty:
            fig, ax = plt.subplots(figsize=(10, 5))
            citations_per_year.plot(kind='bar', color='red', ax=ax)
            ax.set_xlabel("A√±o")
            ax.set_ylabel("Total de citas")
            ax.set_title(f"Citas por a√±o - ID {selected_id}")
            ax.set_xticklabels(citations_per_year.index, rotation=45)
            st.pyplot(fig)

    def plot_publisher_info(publisher_info, selected_id):
        if publisher_info is not None and not publisher_info.empty:
            publisher_info = publisher_info.sort_values(by="num_articles", ascending=False).head(10)

            fig, ax1 = plt.subplots(figsize=(12, 6))  # Ajustar tama√±o para mejor visibilidad en Streamlit

            # Barras para n√∫mero de art√≠culos
            bars = ax1.bar(publisher_info["Publisher"], publisher_info["num_articles"], label="N√∫mero de art√≠culos", alpha=0.7)
            ax1.set_xlabel("Editorial")
            ax1.set_ylabel("N√∫mero de art√≠culos", color="blue")
            ax1.tick_params(axis="y", labelcolor="blue")

            # L√≠nea para n√∫mero de citas
            ax2 = ax1.twinx()
            ax2.plot(publisher_info["Publisher"], publisher_info["total_citations"], marker="o", linestyle="dashed", color="red", label="Total de citas")
            ax2.set_ylabel("Total de citas", color="red")
            ax2.tick_params(axis="y", labelcolor="red")

            plt.title(f"Principales editoriales donde publica ID {selected_id}")

            # Rotar etiquetas en el eje X
            ax1.set_xticklabels(publisher_info["Publisher"], rotation=45, ha="right", fontsize=10)

            # Ajustar margen inferior
            plt.tight_layout()

            # Mostrar gr√°fico en Streamlit
            st.pyplot(fig)


    

    #if uploaded_file:
    df = load_data(uploaded_file)

    # Input para apellido del autor
    author_last_name = st.text_input("Ingresa el apellido del autor")

    if author_last_name:
        available_authors = get_author_options(df, author_last_name)

        if available_authors:
            st.write("Autores encontrados:")
            author_options = {f"ID: {id_} - Nombres: {', '.join(authors)}": id_ for id_, authors in available_authors.items()}
            selected_id = st.selectbox("Seleccione el ID del autor:", list(author_options.keys()))

            if selected_id:
                selected_id = author_options[selected_id]  # Extrae el ID real
                matching_authors = get_authors_by_id(df, selected_id)

                if matching_authors:
                    st.write(f"Autores con ID {selected_id}: {', '.join(matching_authors)}")

                    author_stats = get_author_stats(df, selected_id)
                    st.write(f"**Total de citas asociadas:** {author_stats['total_citations']}")
                    st.write(f"**Total de art√≠culos en los que participa:** {author_stats['total_articles']}")
                    
                    if author_stats['min_year'] and author_stats['max_year']:
                        st.write(f"**A√±o m√°s antiguo de publicaci√≥n:** {author_stats['min_year']}")
                        st.write(f"**A√±o m√°s reciente de publicaci√≥n:** {author_stats['max_year']}")
                        #plot_publications(author_stats['year_counts'], selected_id)

                    if not author_stats['publisher_info'].empty:
                        st.write("**Editoriales en las que ha publicado este ID:**")
                        st.dataframe(author_stats['publisher_info'])
                    
                    
                    print(f"Autores con ID {selected_id}: {', '.join(matching_authors)}")

                    #total_citations = get_total_citations(file_path, selected_id)
                    total_citations = get_total_citations(df, selected_id)
                    print(f"Total de citas asociadas a ID {selected_id}: {total_citations}")

                    #total_articles = get_total_articles(file_path, selected_id)
                    total_articles = get_total_articles(df, selected_id)
                    print(f"Total de art√≠culos en los que participa ID {selected_id}: {total_articles}")

                    min_year, max_year, year_counts, citations_per_year = get_publication_years(file_path, selected_id)
                    if min_year and max_year:
                        print(f"A√±o m√°s antiguo de publicaci√≥n: {min_year}")
                        print(f"A√±o m√°s reciente de publicaci√≥n: {max_year}")
                        plot_publications(year_counts, selected_id)
                        plot_citations_per_year(citations_per_year, selected_id)
                    else:
                        print("No se encontraron a√±os de publicaci√≥n para este autor.")

                    top_articles = get_top_cited_articles(df, selected_id, top_n=10)

                    if top_articles is not None and not top_articles.empty:
                        #st.table(top_articles)
                        top_articles["Year"] = top_articles["Year"].astype(str)  # Convertir a string para evitar comas
                        #top_articles["Year"] = top_articles["Year"].astype(int)  # Forzar tipo entero sin formato
                        st.dataframe(top_articles)

                    else:
                        st.warning("‚ö†Ô∏è No se encontraron art√≠culos con citas registradas para este autor.")
                    publisher_info = get_publisher_info(df, selected_id)
                    if publisher_info is not None and not publisher_info.empty:
                        print("Editoriales en las que ha publicado este ID:")
                        #display(publisher_info)
                        plot_publisher_info(publisher_info, selected_id)                  
                    else:
                        st.warning("No se encontraron editoriales para este autor.")
            else:
                st.warning("No se encontraron autores con ese ID.")
        else:
            st.warning("No se encontraron coincidencias para ese apellido.")

    #def get_author_options(file_path, author_last_name):
    #    df = pd.read_csv(file_path, encoding='utf-8')
    #    if "Authors" not in df.columns or "Author(s) ID" not in df.columns:
    #        print("No se encontraron las columnas necesarias en el archivo.")
    #        return {}

    #    author_dict = {}
    #    for _, row in df.dropna(subset=["Authors", "Author(s) ID"]).iterrows():
    #        authors = row["Authors"].split(";")
    #        ids = str(row["Author(s) ID"]).split(";")
    #        for author, author_id in zip(authors, ids):
    #            author = author.strip()
    #            if author_last_name.lower() in author.lower():
    #                author_dict.setdefault(author_id.strip(), []).append(author)

    #    return author_dict

    #def get_authors_by_id(file_path, selected_id):
    #    df = pd.read_csv(file_path, encoding='utf-8')
    #    if "Authors" not in df.columns or "Author(s) ID" not in df.columns:
    #        print("No se encontraron las columnas necesarias en el archivo.")
    #        return []

    #    matching_authors = set()
    #    for _, row in df.dropna(subset=["Authors", "Author(s) ID"]).iterrows():
    #        authors = row["Authors"].split(";")
    #        ids = str(row["Author(s) ID"]).split(";")
    #        for author, author_id in zip(authors, ids):
    #            if author_id.strip() == selected_id:
    #                matching_authors.add(author.strip())

    #    return list(matching_authors)

    #def get_total_citations(file_path, selected_id):
    #    df = pd.read_csv(file_path, encoding='utf-8')
    #    if "Author(s) ID" not in df.columns or "Cited by" not in df.columns:
    #        print("No se encontraron las columnas necesarias en el archivo.")
    #        return 0#

    #    df_filtered = df[df["Author(s) ID"].str.contains(selected_id, na=False, case=False)]
    #    total_citations = df_filtered["Cited by"].fillna(0).astype(int).sum()

    #    return total_citations

    #def get_total_articles(file_path, selected_id):
    #    df = pd.read_csv(file_path, encoding='utf-8')
    #    if "Author(s) ID" not in df.columns:
    #        print("No se encontraron las columnas necesarias en el archivo.")
    #        return 0

    #    df_filtered = df[df["Author(s) ID"].str.contains(selected_id, na=False, case=False)]
    #    total_articles = df_filtered.shape[0]

    #    return total_articles

    #def get_publication_years(file_path, selected_id):
    #    df = pd.read_csv(file_path, encoding='utf-8')
    #    if "Author(s) ID" not in df.columns or "Year" not in df.columns:
    #        print("No se encontraron las columnas necesarias en el archivo.")
    #        return None, None, None, None

    #    df_filtered = df[df["Author(s) ID"].str.contains(selected_id, na=False, case=False)]
    #    years = df_filtered["Year"].dropna().astype(int)
    #    if years.empty:
    #        return None, None, None, None

    #    min_year = years.min()
    #    max_year = years.max()
    #    year_counts = years.value_counts().sort_index()

    #    # Total de citas por a√±o
    #    citations_per_year = df_filtered.groupby("Year")["Cited by"].sum().fillna(0).astype(int)

    #    return min_year, max_year, year_counts, citations_per_year

    #def get_publisher_info(file_path, selected_id):
    #    df = pd.read_csv(file_path, encoding='utf-8')
    #    if "Author(s) ID" not in df.columns or "Publisher" not in df.columns or "Cited by" not in df.columns:
    #        print("No se encontraron las columnas necesarias en el archivo.")
    #        return None

    #    df_filtered = df[df["Author(s) ID"].str.contains(selected_id, na=False, case=False)]
    #    publisher_stats = df_filtered.groupby("Publisher").agg(
    #        num_articles=("Title", "count"),
    #        total_citations=("Cited by", "sum")
    #    ).reset_index()

    #    return publisher_stats

    def plot_publications(year_counts, selected_id):
        if year_counts is not None and not year_counts.empty:
            fig, ax = plt.subplots(figsize=(10, 5))
            year_counts.plot(kind='bar', color='blue', ax=ax)
            ax.set_xlabel("A√±o de publicaci√≥n")
            ax.set_ylabel("N√∫mero de publicaciones")
            ax.set_title(f"Publicaciones por a√±o - ID {selected_id}")
            ax.set_xticklabels(year_counts.index, rotation=45)
            st.pyplot(fig)

    def plot_citations_per_year(citations_per_year, selected_id):
        if citations_per_year is not None and not citations_per_year.empty:
            fig, ax = plt.subplots(figsize=(10, 5))
            citations_per_year.plot(kind='bar', color='red', ax=ax)
            ax.set_xlabel("A√±o")
            ax.set_ylabel("Total de citas")
            ax.set_title(f"Citas por a√±o - ID {selected_id}")
            ax.set_xticklabels(citations_per_year.index, rotation=45)
            st.pyplot(fig)


    

    def plot_publisher_info(publisher_info, selected_id):
        if publisher_info is not None and not publisher_info.empty:
            publisher_info = publisher_info.sort_values(by="num_articles", ascending=False).head(10)

            fig, ax1 = plt.subplots(figsize=(12, 6))  # Ajustar tama√±o para mejor visibilidad en Streamlit

            # Barras para n√∫mero de art√≠culos
            bars = ax1.bar(publisher_info["Publisher"], publisher_info["num_articles"], label="N√∫mero de art√≠culos", alpha=0.7)
            ax1.set_xlabel("Editorial")
            ax1.set_ylabel("N√∫mero de art√≠culos", color="blue")
            ax1.tick_params(axis="y", labelcolor="blue")

            # L√≠nea para n√∫mero de citas
            ax2 = ax1.twinx()
            ax2.plot(publisher_info["Publisher"], publisher_info["total_citations"], marker="o", linestyle="dashed", color="red", label="Total de citas")
            ax2.set_ylabel("Total de citas", color="red")
            ax2.tick_params(axis="y", labelcolor="red")

            plt.title(f"Principales editoriales donde publica ID {selected_id}")

            # Rotar etiquetas en el eje X
            ax1.set_xticklabels(publisher_info["Publisher"], rotation=45, ha="right", fontsize=10)

            # Ajustar margen inferior
            plt.tight_layout()

            # Mostrar gr√°fico en Streamlit
            st.pyplot(fig)


    import streamlit as st
    import pandas as pd
    import re
    import seaborn as sns
    import matplotlib.pyplot as plt

    def process_author_data(file):
        #df = pd.read_csv(file, encoding='utf-8')
        df = load_data(uploaded_file)
        df.columns = df.columns.str.strip().str.replace(" ", "_")  # Reemplazar espacios en nombres de columnas

        # Verificar la existencia de las columnas necesarias
        required_columns = ["Author_full_names", "Author(s)_ID", "Cited_by"]
        if not all(col in df.columns for col in required_columns):
            st.error(f"Faltan columnas necesarias en el archivo: {', '.join(required_columns)}")
            return None

        # Crear un diccionario con nombres completos y sus IDs
        author_id_map = {}
        for row in df.dropna(subset=["Author_full_names"]).itertuples(index=False):
            author_entries = getattr(row, "Author_full_names").split(";")
            for entry in author_entries:
                match = re.match(r"(.*) \((\d+)\)", entry.strip())
                if match:
                    name, author_id = match.groups()
                    author_id = str(author_id)  # Convertir a string
                    author_id_map.setdefault(author_id, []).append(name)

        # Asignar el nombre m√°s frecuente a cada ID
        author_name_map = {id_: max(set(names), key=names.count) for id_, names in author_id_map.items()}

        # Expandir filas con m√∫ltiples IDs
        df = df.assign(**{"Author(s)_ID": df["Author(s)_ID"].str.split(";")}).explode("Author(s)_ID")
        df["Author(s)_ID"] = df["Author(s)_ID"].str.strip()

        # Mapear nombres a los IDs de autores
        df["Authors"] = df["Author(s)_ID"].map(author_name_map)
        return df

    def build_author_collaboration_matrix(df, selected_author_id):
        if "Author(s)_ID" not in df.columns:
            st.error("No se encontr√≥ la columna 'Author(s)_ID' en el archivo.")
            return None

        df_filtered = df[df["Author(s)_ID"] == selected_author_id]

        collaboration_counts = {}
        for _, row in df_filtered.iterrows():
            coauthors = df[df["Title"] == row["Title"]]["Author(s)_ID"].unique()
            for coauthor in coauthors:
                if coauthor != selected_author_id:
                    collaboration_counts[coauthor] = collaboration_counts.get(coauthor, 0) + 1

        return collaboration_counts

    def build_author_citation_matrix(df, selected_author_id):
        if "Author(s)_ID" not in df.columns or "Cited_by" not in df.columns:
            st.error("No se encontraron las columnas necesarias en el archivo.")
            return None

        df_filtered = df[df["Author(s)_ID"] == selected_author_id]

        citation_counts = {}
        for _, row in df_filtered.iterrows():
            coauthors = df[df["Title"] == row["Title"]]["Author(s)_ID"].unique()
            citations = row["Cited_by"] if not pd.isna(row["Cited_by"]) else 0
            for coauthor in coauthors:
                if coauthor != selected_author_id:
                    citation_counts[coauthor] = citation_counts.get(coauthor, 0) + citations

        return citation_counts

    def plot_heatmap(data_dict, title, ylabel, cmap="Blues"):
        if not data_dict:
            st.warning(f"No hay suficientes datos para generar {title}.")
            return

        df_heatmap = pd.DataFrame(list(data_dict.items()), columns=["Coautor_ID", ylabel])
        df_heatmap = df_heatmap.sort_values(by=ylabel, ascending=False).head(20)

        fig, ax = plt.subplots(figsize=(14, 6))
        sns.heatmap(df_heatmap.set_index("Coautor_ID").T, annot=True, fmt=".0f", cmap=cmap, cbar=True, linewidths=0.5, ax=ax)
        ax.set_title(title)
        ax.set_xlabel("Coautores")
        ax.set_ylabel("")

        st.pyplot(fig)

    # **Interfaz en Streamlit**
    st.title("An√°lisis de Redes de Colaboraci√≥n y Citas Acad√©micas")

    #uploaded_file = st.file_uploader("Cargue el archivo CSV con los datos de autores", type=["csv"])

    if uploaded_file:
        #df=load_data(uploaded_file)
        df = process_author_data(uploaded_file)
    
        if df is not None:
            st.success("Datos cargados exitosamente.")

            # Selecci√≥n de autor
            unique_authors = df["Author(s)_ID"].dropna().unique().tolist()
            selected_author_id = st.selectbox("Seleccione un ID de autor:", unique_authors)
            st.session_state["selected_author_id"] = selected_author_id

            
            if selected_author_id:
                st.subheader(f"Mapas de Calor para el ID: {selected_author_id}")

                # **Mapa de colaboraciones (Publicaciones compartidas)**
                collaboration_counts = build_author_collaboration_matrix(df, selected_author_id)
                plot_heatmap(collaboration_counts, f"Colaboraciones m√°s frecuentes con {selected_author_id}", "Publicaciones Conjuntas", cmap="Blues")

                # **Mapa de citas (Citas compartidas entre coautores)**
                citation_counts = build_author_citation_matrix(df, selected_author_id)
                plot_heatmap(citation_counts, f"Citas recibidas por colaboraciones con {selected_author_id}", "Total Citas", cmap="Reds")
################################################################################################################



##############################################################

    import streamlit as st
    import pandas as pd
    import re
    import matplotlib.pyplot as plt
    import numpy as np
    from wordcloud import WordCloud, STOPWORDS
    from sklearn.feature_extraction.text import TfidfVectorizer
    from scipy.spatial.distance import cosine

    # --- PROCESAR ARCHIVO ---
    def process_author_data(df):
        """ Procesa el DataFrame cargado y estructura los datos de autores. """
        if df is None or df.empty:
            st.error("‚ùå No se pudo procesar el archivo. Verifica su contenido.")
            return None

        df.columns = df.columns.str.strip().str.replace(" ", "_")

        required_columns = ["Author_full_names", "Author(s)_ID", "Title"]
        if not all(col in df.columns for col in required_columns):
            st.error(f"‚ùå Faltan columnas necesarias en el archivo: {', '.join(required_columns)}")
            return None

        author_id_map = {}
        for row in df.dropna(subset=["Author_full_names"]).itertuples(index=False):
            author_entries = getattr(row, "Author_full_names").split(";")
            for entry in author_entries:
                match = re.match(r"(.*) \((\d+)\)", entry.strip())
                if match:
                    name, author_id = match.groups()
                    author_id = str(author_id)
                    author_id_map.setdefault(author_id, []).append(name)
    
        author_name_map = {id_: max(set(names), key=names.count) for id_, names in author_id_map.items()}

        df["Author(s)_ID"] = df["Author(s)_ID"].str.split(";")
        df = df.explode("Author(s)_ID")
        df["Author(s)_ID"] = df["Author(s)_ID"].str.strip()
        df["Authors"] = df["Author(s)_ID"].map(author_name_map)

        return df

    # --- EXTRAER TITULOS POR AUTOR ---
    def extract_author_titles(df, selected_author_id):
        return df[df["Author(s)_ID"] == selected_author_id]["Title"].dropna().tolist()

    # --- GENERAR NUBE DE PALABRAS ---
    def generate_wordcloud(text, selected_author_id):
        if not text:
            st.warning("‚ö†Ô∏è No hay t√≠tulos de publicaciones disponibles para generar la nube de palabras.")
            return

        stopwords = set(STOPWORDS)
        stopwords.update([
        "study", "analysis", "using", "approach", "model", "method", "based", "review", "system",
        "estudio", "an√°lisis", "uso", "enfoque", "modelo", "m√©todo", "basado", "revisi√≥n", "sistema",
        "effect", "impact", "influence", "role", "characteristics", "performance",
        "efecto", "impacto", "influencia", "rol", "caracter√≠sticas", "desempe√±o",
        "case", "cases", "example", "examples", "context",
        "caso", "casos", "ejemplo", "ejemplos", "contexto",
        "comparison", "relation", "relationship", "association", "between",
        "comparaci√≥n", "relaci√≥n", "asociaci√≥n", "entre", "de", "en", "y", "con"
        ])

        wordcloud = WordCloud(width=800, height=400, background_color='white', colormap='coolwarm', stopwords=stopwords).generate(" ".join(text))

        fig, ax = plt.subplots(figsize=(10, 5))
        ax.imshow(wordcloud, interpolation='bilinear')
        ax.axis("off")
        ax.set_title(f"Temas centrales en las publicaciones de {selected_author_id}", fontsize=14)
        st.pyplot(fig)

    # --- CALCULAR DIVERSIDAD L√âXICA ---
    def compute_lexical_diversity(titles):
        words = " ".join(titles).split()
        if not words:
            st.warning("‚ö†Ô∏è No hay palabras suficientes para calcular la diversidad l√©xica.")
            return None, None

        unique_words = set(words)

        shannon_entropy = -sum((words.count(word) / len(words)) * np.log2(words.count(word) / len(words)) for word in unique_words)
        simpson_index = sum((words.count(word) / len(words)) ** 2 for word in unique_words)

        return shannon_entropy, simpson_index

    # --- SIMILITUD TEM√ÅTICA ---
    def topic_clustering(titles):
        if not titles:
            st.warning("‚ö†Ô∏è No hay t√≠tulos suficientes para calcular similitud tem√°tica.")
            return None

        vectorizer = TfidfVectorizer(stop_words='english')
        tfidf_matrix = vectorizer.fit_transform(titles)

        if tfidf_matrix.shape[0] < 2:
            st.warning("‚ö†Ô∏è Se necesita m√°s de un t√≠tulo para calcular similitud tem√°tica.")
            return None

        similarity_matrix = cosine(tfidf_matrix.toarray().mean(axis=0), tfidf_matrix.toarray().mean(axis=0))
        return 1 - similarity_matrix

    # --- INTERFAZ EN STREAMLIT ---
    st.title("üìä An√°lisis de Multidisciplinariedad en Publicaciones")
    import streamlit as st
    import pandas as pd

    @st.cache_data
    def load_data(file):
        df = pd.read_csv(file, encoding='utf-8')
        return df

    #uploaded_file = st.file_uploader("üìÇ Carga un archivo CSV con los datos de autores", type=["csv"])

    if uploaded_file:
        df = load_data(uploaded_file)
        #df = pd.read_csv(uploaded_file, encoding='utf-8')  # Cargar archivo CSV
        df = process_author_data(df)  # Procesar datos

        if df is not None and not df.empty:
            st.success("‚úÖ Datos cargados y procesados correctamente.")

            # Selecci√≥n de autor
            unique_authors = df["Author(s)_ID"].dropna().unique().tolist()
            #selected_author_id = st.selectbox("üîç Selecciona un ID de autor:", unique_authors)

            if selected_author_id:
                st.subheader(f"üìñ An√°lisis de publicaciones del autor: {selected_author_id}")

                # Obtener t√≠tulos del autor
                author_titles = extract_author_titles(df, selected_author_id)

                # **Generar nube de palabras**
                st.subheader("‚òÅÔ∏è Nube de palabras de t√≠tulos de publicaciones")
                generate_wordcloud(author_titles, selected_author_id)

                # **Calcular diversidad l√©xica**
                st.subheader("üìä Diversidad L√©xica")
                shannon_entropy, simpson_index = compute_lexical_diversity(author_titles)
                if shannon_entropy and simpson_index:
                    st.write(f"**üîπ √çndice de Shannon:** {shannon_entropy:.4f}")
                    st.write(f"**üîπ √çndice de Simpson:** {simpson_index:.4f}")

                # **Calcular similitud tem√°tica**
                st.subheader("üìà Similitud Tem√°tica entre T√≠tulos")
                similarity_score = topic_clustering(author_titles)
                if similarity_score:
                    st.write(f"**üîπ Similitud Tem√°tica Promedio:** {similarity_score:.4f}")
        else:
            st.error("‚ùå No se pudieron procesar los datos. Verifica el contenido del archivo.")


    
#########################################################3333


    import streamlit as st
    import pandas as pd
    import numpy as np
    import re
    import matplotlib.pyplot as plt
    import plotly.express as px
    from wordcloud import WordCloud
    from sklearn.feature_extraction.text import TfidfVectorizer
    from sklearn.decomposition import PCA
    from sklearn.cluster import KMeans

    # Procesamiento de datos de autores
    def process_author_data(df):
        df.columns = df.columns.str.strip().str.replace(" ", "_")
        if "Author_full_names" not in df.columns or "Author(s)_ID" not in df.columns or "Title" not in df.columns:
            st.error("No se encontraron las columnas necesarias en el archivo.")
            return None
        df["Author(s)_ID"] = df["Author(s)_ID"].str.split(";")
        df = df.explode("Author(s)_ID")
        df["Author(s)_ID"] = df["Author(s)_ID"].str.strip()
        return df

    # Extracci√≥n de t√≠tulos por autor
    def extract_author_titles(df, selected_author_id):
        return df[df["Author(s)_ID"] == selected_author_id][["Title"]].dropna()

    # Clustering de temas utilizando K-Means
    def topic_clustering_kmeans(titles_df):
        titles = titles_df["Title"].tolist()
        vectorizer = TfidfVectorizer(stop_words='english')
        tfidf_matrix = vectorizer.fit_transform(titles)
        num_clusters = min(5, len(titles))  # M√°ximo 5 clusters o el n√∫mero de t√≠tulos disponibles
        kmeans = KMeans(n_clusters=num_clusters, random_state=42, n_init=10)
        labels = kmeans.fit_predict(tfidf_matrix)
        titles_df["Cluster"] = labels
        return titles_df, vectorizer, tfidf_matrix, labels, num_clusters

    # Visualizaci√≥n de distribuci√≥n de temas con PCA
    def plot_topic_distribution(titles_df, labels, num_clusters):
        vectorizer = TfidfVectorizer(stop_words='english')
        tfidf_matrix = vectorizer.fit_transform(titles_df["Title"])
        pca = PCA(n_components=2)
        reduced_data = pca.fit_transform(tfidf_matrix.toarray())
        titles_df["Componente 1"] = reduced_data[:, 0]
        titles_df["Componente 2"] = reduced_data[:, 1]
        titles_df["Cluster"] = labels
        fig = px.scatter(
            titles_df, x="Componente 1", y="Componente 2", color=titles_df["Cluster"].astype(str),
            title="Mapa de Similitud Tem√°tica (PCA)",
            hover_data={"Title": True, "Componente 1": False, "Componente 2": False, "Cluster": True},
            color_discrete_sequence=px.colors.qualitative.Dark24[:num_clusters]
        )
        st.plotly_chart(fig)

    # Generaci√≥n de nubes de palabras por cluster
    def plot_wordclouds_by_cluster(titles_df, num_clusters):
        for cluster in range(num_clusters):
            titles_in_cluster = titles_df[titles_df["Cluster"] == cluster]["Title"].tolist()
            text = " ".join(titles_in_cluster)
            wordcloud = WordCloud(width=800, height=400, background_color="white", colormap="Dark2").generate(text)
            fig, ax = plt.subplots(figsize=(8, 4))
            ax.imshow(wordcloud, interpolation="bilinear")
            ax.axis("off")
            ax.set_title(f"Nube de Palabras - Cluster {cluster}")
            st.pyplot(fig)

    # Distribuci√≥n de clusters (histograma)
    def plot_cluster_distribution(titles_df, num_clusters):
        df_clusters = pd.DataFrame({"Cluster": titles_df["Cluster"]})
        fig = px.histogram(df_clusters, x="Cluster", nbins=num_clusters, color="Cluster",
                       color_discrete_sequence=px.colors.qualitative.Dark24[:num_clusters])
        fig.update_layout(title="Distribuci√≥n de Temas (Clusters K-Means)",
                      xaxis_title="Cluster",
                      yaxis_title="N√∫mero de Art√≠culos",
                      bargap=0.2)
        st.plotly_chart(fig)

    # C√°lculo de diversidad l√©xica
    def compute_lexical_diversity(titles_df):
        words = " ".join(titles_df["Title"]).split()
        unique_words = set(words)
        shannon_entropy = -sum((words.count(word) / len(words)) * np.log2(words.count(word) / len(words)) for word in unique_words)
        simpson_index = sum((words.count(word) / len(words)) ** 2 for word in unique_words)
        return shannon_entropy, simpson_index

    # Gr√°fico de diversidad l√©xica
    def plot_diversity_metrics(shannon_entropy, simpson_index):
        df_diversity = pd.DataFrame({"√çndice": ["Shannon", "Simpson"], "Valor": [shannon_entropy, simpson_index]})
        fig = px.bar(df_diversity, x="√çndice", y="Valor", color="√çndice",
                 title="√çndices de Diversidad L√©xica",
                 color_discrete_sequence=px.colors.qualitative.Dark24[:2])
        fig.update_layout(yaxis_title="Valor de Diversidad")
        st.plotly_chart(fig)

    # Aplicaci√≥n principal de Streamlit
    st.title("An√°lisis de Multidisciplinariedad en Publicaciones")

    #uploaded_file = st.file_uploader("Archivo CSV con los datos de autores", type=["csv"])

    if uploaded_file:
        df=load_data(uploaded_file)
        #df = pd.read_csv(uploaded_file, encoding='utf-8')
        df = process_author_data(df)

        if df is not None:
            st.success("Datos cargados exitosamente.")

            unique_authors = df["Author(s)_ID"].dropna().unique().tolist()
            #selected_author_id = st.selectbox("Selecciona un ID:", unique_authors)

            if selected_author_id:
                st.subheader(f"An√°lisis de publicaciones del autor: {selected_author_id}")

                author_titles_df = extract_author_titles(df, selected_author_id)

                if not author_titles_df.empty:
                    clustered_df, vectorizer, tfidf_matrix, labels, num_clusters = topic_clustering_kmeans(author_titles_df)

                    st.subheader("Mapa de Similitud Tem√°tica (PCA)")
                    plot_topic_distribution(clustered_df, labels, num_clusters)

                    st.subheader("Distribuci√≥n de Temas (Clusters K-Means)")
                    plot_cluster_distribution(clustered_df, num_clusters)

                    st.subheader("Nubes de Palabras por Cluster")
                    plot_wordclouds_by_cluster(clustered_df, num_clusters)

                    st.subheader("√çndices de Diversidad L√©xica")
                    shannon_entropy, simpson_index = compute_lexical_diversity(clustered_df)
                    plot_diversity_metrics(shannon_entropy, simpson_index)
                else:
                    st.warning(f"No se encontraron t√≠tulos para el autor ID: {selected_author_id}")


    import streamlit as st
    import pandas as pd
    import numpy as np
    import re
    import matplotlib.pyplot as plt
    import seaborn as sns
    import plotly.express as px
    from wordcloud import WordCloud
    from sklearn.feature_extraction.text import TfidfVectorizer
    from sklearn.cluster import KMeans
    from sklearn.tree import DecisionTreeClassifier
    from sklearn import tree
    from sklearn.model_selection import GridSearchCV, cross_val_score
    from sklearn.feature_selection import SelectFromModel
    from sklearn.ensemble import BaggingClassifier
    from xgboost import XGBClassifier

    # Funci√≥n para procesar los datos de autores
    def process_author_data(df):
        df.columns = df.columns.str.strip().str.replace(" ", "_")
        if "Author_full_names" not in df.columns or "Author(s)_ID" not in df.columns or "Title" not in df.columns:
            st.error("No se encontraron las columnas necesarias en el archivo.")
            return None
        df["Author(s)_ID"] = df["Author(s)_ID"].str.split(";")
        df = df.explode("Author(s)_ID")
        df["Author(s)_ID"] = df["Author(s)_ID"].str.strip()
        return df

    # Extraer t√≠tulos de un autor espec√≠fico
    def extract_author_titles(df, selected_author_id):
        return df[df["Author(s)_ID"] == selected_author_id][["Title", "Cited_by"]].dropna()

    # Clustering de art√≠culos mediante K-Means
    def topic_clustering_kmeans(df, titles):
        vectorizer = TfidfVectorizer(stop_words='english')
        tfidf_matrix = vectorizer.fit_transform(titles)
        num_clusters = min(5, len(titles))  # M√°ximo 5 clusters
        kmeans = KMeans(n_clusters=num_clusters, random_state=42, n_init=10)
        labels = kmeans.fit_predict(tfidf_matrix)

        df_clustered = df.copy()
        df_clustered["Cluster"] = labels

        # Asignar nombres a los clusters
        terms = np.array(vectorizer.get_feature_names_out())
        cluster_names = {}
        for i in range(num_clusters):
            cluster_center = kmeans.cluster_centers_[i]
            top_terms = terms[np.argsort(cluster_center)[-3:]]  # Top 3 palabras clave
            cluster_names[i] = " - ".join(top_terms)

        df_clustered["Cluster Name"] = df_clustered["Cluster"].map(cluster_names)
        df_clustered["Cited_by"] = pd.to_numeric(df_clustered["Cited_by"], errors='coerce').fillna(0)

        return tfidf_matrix, labels, vectorizer, cluster_names, df_clustered

    # Ajuste de hiperpar√°metros y entrenamiento del √°rbol de decisi√≥n
    def train_decision_tree(tfidf_matrix, labels):
        param_grid = {
            'max_depth': [3, 5, 10, None],
            'min_samples_split': [2, 5, 10],
            'min_samples_leaf': [1, 2, 4],
            'ccp_alpha': [0.0, 0.01, 0.1]
        }

        clf = DecisionTreeClassifier(random_state=42)
        grid_search = GridSearchCV(clf, param_grid, cv=5, scoring='accuracy')
        grid_search.fit(tfidf_matrix.toarray(), labels)

        best_clf = grid_search.best_estimator_
        best_params = grid_search.best_params_
        return best_clf, best_params

    # Visualizaci√≥n del √°rbol de decisi√≥n
    def plot_decision_tree(clf, vectorizer, cluster_names):
        fig, ax = plt.subplots(figsize=(12, 6))
        tree.plot_tree(clf, feature_names=vectorizer.get_feature_names_out(),
                   class_names=[cluster_names[i] for i in set(clf.classes_)],
                   filled=True, rounded=True, ax=ax)
        plt.title("√Årbol de Decisi√≥n para Clasificaci√≥n de Art√≠culos")
        st.pyplot(fig)

    # Ensamblado con Bagging
    def train_bagging_classifier(tfidf_matrix, labels):
        base_clf = DecisionTreeClassifier(random_state=42)
        bagging_clf = BaggingClassifier(estimator=base_clf, n_estimators=100, random_state=42)
        bagging_clf.fit(tfidf_matrix.toarray(), labels)
        return bagging_clf

    # Ensamblado con XGBoost
    def train_xgboost_classifier(tfidf_matrix, labels):
        xgb_clf = XGBClassifier(random_state=42, n_estimators=100, learning_rate=0.1, max_depth=5)
        xgb_clf.fit(tfidf_matrix.toarray(), labels)
        return xgb_clf

    # Selecci√≥n de caracter√≠sticas
    def feature_selection(tfidf_matrix, labels):
        clf = DecisionTreeClassifier(random_state=42)
        clf.fit(tfidf_matrix.toarray(), labels)
        selector = SelectFromModel(clf, prefit=True)
        X_selected = selector.transform(tfidf_matrix.toarray())
        return X_selected

    # Streamlit Application
    st.title("An√°lisis de Clustering y √Årboles de Decisi√≥n")

    #uploaded_file = st.file_uploader("Sube el archivo CSV de Scopus", type="csv")

    if uploaded_file is not None:
        df = load_data(uploaded_file)

        #df = pd.read_csv(uploaded_file, encoding='utf-8')
        df = process_author_data(df)

        if df is not None:
            st.success("Datos cargados exitosamente.")

            author_ids = df["Author(s)_ID"].unique().tolist()
            #selected_author_id = st.selectbox("Selecciona el ID del autor para analizar", author_ids)

            if selected_author_id:
                st.subheader(f"An√°lisis de publicaciones del autor: {selected_author_id}")

                author_data = extract_author_titles(df, selected_author_id)

                if not author_data.empty:
                    tfidf_matrix, labels, vectorizer, cluster_names, df_clustered = topic_clustering_kmeans(author_data, author_data["Title"].tolist())

                    st.subheader("Palabras clave por cluster:")
                    for cluster, keywords in cluster_names.items():
                        st.write(f"**Cluster {cluster}:** {keywords}")

                    st.subheader("N√∫mero de citas por cluster:")
                    citations_by_cluster = df_clustered.groupby("Cluster Name")["Cited_by"].sum()
                    st.bar_chart(citations_by_cluster)

                    # Entrenar √°rbol de decisi√≥n con hiperpar√°metros √≥ptimos
                    best_clf, best_params = train_decision_tree(tfidf_matrix, labels)
                    st.subheader(f"√Årbol de Decisi√≥n con hiperpar√°metros optimizados")
                    plot_decision_tree(best_clf, vectorizer, cluster_names)

                    # Evaluaci√≥n con validaci√≥n cruzada
                    scores = cross_val_score(best_clf, tfidf_matrix.toarray(), labels, cv=5, scoring='accuracy')
                    st.subheader(f"Precisi√≥n promedio con validaci√≥n cruzada: {scores.mean():.4f}")

                    # Entrenar modelos de ensamblado
                    #st.subheader("Ensamblado con Bagging")
                    #bagging_clf = train_bagging_classifier(tfidf_matrix, labels)
                    #st.write("Modelo Bagging entrenado.")

                    #st.subheader("Ensamblado con XGBoost")
                    #xgb_clf = train_xgboost_classifier(tfidf_matrix, labels)
                    #st.write("Modelo XGBoost entrenado.")

                    # Selecci√≥n de caracter√≠sticas
                    #st.subheader("Selecci√≥n de Caracter√≠sticas")
                    #X_selected = feature_selection(tfidf_matrix, labels)
                    #st.write(f"N√∫mero de caracter√≠sticas seleccionadas: {X_selected.shape[1]}")

                else:
                    st.warning(f"No se encontraron t√≠tulos para el autor ID: {selected_author_id}")


#######################################################################################

    import streamlit as st
    import pandas as pd
    import itertools
    import networkx as nx
    import plotly.graph_objects as go
    from collections import Counter

    # --- FUNCI√ìN PARA OBTENER AUTORES POR APELLIDO ---
    def get_author_options(df, author_last_name):
        """Devuelve un diccionario {ID: Nombre m√°s com√∫n} para un apellido dado."""
        if "Authors" not in df.columns or "Author(s) ID" not in df.columns:
            return {}

        author_dict = {}
        for _, row in df.dropna(subset=["Authors", "Author(s) ID"]).iterrows():
            authors = row["Authors"].split(";")
            ids = str(row["Author(s) ID"]).split(";")
            for author, author_id in zip(authors, ids):
                author = author.strip()
                author_id = author_id.strip()
                if author_last_name.lower() in author.lower():
                    author_dict.setdefault(author_id, []).append(author)

        return {author_id: Counter(names).most_common(1)[0][0] for author_id, names in author_dict.items()}

    # --- FUNCI√ìN PARA CREAR MAPEO ID -> NOMBRE ---
    def create_id_to_name_mapping(df):
        """Crea un diccionario {ID: Nombre m√°s com√∫n del autor}."""
        if "Authors" not in df.columns or "Author(s) ID" not in df.columns:
            return {}

        id_to_name = {}
        for _, row in df.dropna(subset=["Authors", "Author(s) ID"]).iterrows():
            authors = row["Authors"].split(";")
            ids = str(row["Author(s) ID"]).split(";")
            for author, author_id in zip(authors, ids):
                author = author.strip()
                author_id = author_id.strip()
                id_to_name.setdefault(author_id, []).append(author)

        return {author_id: Counter(names).most_common(1)[0][0] for author_id, names in id_to_name.items()}

    # --- FUNCI√ìN PARA GENERAR RED DE COLABORACI√ìN ---
    def visualize_collaboration_network(df, selected_author_id, id_to_name, selected_year):
        """Genera una red de colaboraci√≥n en Plotly con relaci√≥n de aspecto equilibrada."""

        # Si se elige "Todos los a√±os", generar redes para cada a√±o individualmente
        if selected_year == "Todos los a√±os":
            years = sorted(df["Year"].dropna().astype(int).unique())
            for year in years:
                st.subheader(f"üîó Red de colaboraci√≥n en {year}")
                visualize_collaboration_network(df[df["Year"] == year], selected_author_id, id_to_name, year)
            return

        # Filtrar el DataFrame por el a√±o seleccionado
        df_filtered = df[df["Year"] == selected_year]

        if df_filtered.empty:
            st.warning(f"No se encontraron publicaciones para el autor con ID: {selected_author_id}")
            return

        # Crear la red de colaboraci√≥n
        G = nx.Graph()
        for _, row in df_filtered.iterrows():
            coauthors = row["Author(s) ID"].split(";")
            coauthors = [author.strip() for author in coauthors if author]

            for i in range(len(coauthors)):
                for j in range(i + 1, len(coauthors)):
                    G.add_edge(coauthors[i], coauthors[j])

        if len(G.nodes) == 0:
            st.warning("‚ö†Ô∏è No hay colaboraciones registradas en este per√≠odo.")
            return

        # Ajustar la distribuci√≥n de nodos para evitar estiramiento
        pos = nx.spring_layout(G, seed=42, scale=1.5)

        # Crear trazas de bordes (edges)
        edge_trace = go.Scatter(
            x=[], y=[], line=dict(width=1.5, color="black"),  # Bordes negros
            hoverinfo="none", mode="lines"
        )

        for edge in G.edges():
            x0, y0 = pos[edge[0]]
            x1, y1 = pos[edge[1]]
            edge_trace.x += (x0, x1, None)
            edge_trace.y += (y0, y1, None)
    
        # Crear trazas de nodos (nodes)
        node_x = []
        node_y = []
        node_color = []
        node_texts = []

        for node in G.nodes():
            x, y = pos[node]
            node_x.append(x)
            node_y.append(y)
            node_color.append("red" if node == selected_author_id else "blue")  # Autor principal en rojo
            most_common_name = id_to_name.get(node, "Nombre no disponible")
            node_texts.append(f"ID: {node}<br>Nombre: {most_common_name}")

        node_trace = go.Scatter(
            x=node_x, y=node_y, mode="markers",
            marker=dict(size=15, color=node_color, opacity=0.8),
            text=node_texts, hoverinfo="text"
        )

        # Crear figura en Plotly con relaci√≥n de aspecto equilibrada
        fig = go.Figure(data=[edge_trace, node_trace])
        fig.update_layout(
            title=f"Red de Colaboraci√≥n en {selected_year}",
            showlegend=False, hovermode="closest",
            autosize=True,  # Ajuste autom√°tico del tama√±o
            margin=dict(l=40, r=40, t=50, b=50),  # M√°rgenes m√°s equilibrados
            xaxis=dict(showgrid=False, zeroline=False, scaleanchor='y', constrain="domain"),  
            yaxis=dict(showgrid=False, zeroline=False, constrain="domain")
        )

        # Mostrar la gr√°fica en Streamlit
        st.plotly_chart(fig)

    # --- INTERFAZ EN STREAMLIT ---
    st.title("üìä An√°lisis de Redes de Colaboraci√≥n en Publicaciones")

    uploaded_file = st.file_uploader("üìÇ Cargue un archivo CSV con datos de autores", type=["csv"])

    if uploaded_file:
        df = pd.read_csv(uploaded_file, encoding='utf-8')  # Cargar datos
        id_to_name = create_id_to_name_mapping(df)  # Crear mapeo ID -> Nombre

        # --- INPUT PARA FILTRAR POR APELLIDO ---
        author_last_name = st.text_input("üîé Ingresar el apellido del autor:")

        if author_last_name:
            available_authors = get_author_options(df, author_last_name)

            if available_authors:
                # --- SELECCI√ìN DEL AUTOR EN `st.selectbox` ---
                selected_id = st.selectbox(
                    "üéØ Seleccion del autor:",
                    options=list(available_authors.keys()),
                    format_func=lambda x: f"{available_authors[x]} (ID: {x})"  # Muestra nombre e ID en el men√∫
                )

                if selected_id:
                    df_filtered = df[df["Author(s) ID"].str.contains(selected_id, na=False, case=False)]
                    years = sorted(df_filtered["Year"].dropna().astype(int).unique())

                    # --- SELECCI√ìN DEL A√ëO ---
                    if years:
                        selected_year = st.selectbox("üìÖ A√±o de colaboraci√≥n:", ["Todos los a√±os"] + years)

                        # --- BOT√ìN PARA GENERAR RED ---
                        if st.button("üîó Red de Colaboraci√≥n"):
                            visualize_collaboration_network(df_filtered, selected_id, id_to_name, selected_year)
                    else:
                        st.warning("‚ö†Ô∏è No se encontraron publicaciones con a√±os registrados.")
            else:
                st.warning("‚ö†Ô∏è No se encontraron coincidencias para ese apellido.")


####################################################3

    import imageio
    import tempfile
    import os
    import io


    def generate_collaboration_gif(df, selected_id, id_to_name):
        """Genera un GIF mostrando la evoluci√≥n de la red de colaboraci√≥n a√±o con a√±o."""
        st.subheader("üé• Evoluci√≥n de la Red de Colaboraci√≥n (GIF)")
        years = sorted(df["Year"].dropna().astype(int).unique())
        image_list = []  # Lista para almacenar im√°genes en memoria

        # Crear una red global para fijar las posiciones de los nodos
        G_global = nx.Graph()
        for _, row in df.iterrows():
            coauthors = row["Author(s) ID"].split(";")
            for i in range(len(coauthors)):
                for j in range(i + 1, len(coauthors)):
                    G_global.add_edge(coauthors[i].strip(), coauthors[j].strip())
    
        fixed_pos = nx.spring_layout(G_global, seed=42)

        # Generar los grafos a√±o por a√±o y guardarlos como im√°genes
        for year in years:
            #fig, _ = visualize_collaboration_network(df, selected_id, id_to_name, year, fixed_pos)
            fig, _ = visualize_collaboration_network(df_filtered, selected_id, id_to_name, selected_year)

            # Guardar la imagen en memoria
            temp_img_path = tempfile.NamedTemporaryFile(delete=False, suffix=".png").name
            fig.write_image(temp_img_path, format="png", width=800, height=600)
            image_list.append(imageio.imread(temp_img_path))
            os.remove(temp_img_path)

        # Crear un archivo temporal para guardar el GIF
        temp_gif_path = tempfile.NamedTemporaryFile(delete=False, suffix=".gif").name
        imageio.mimsave(temp_gif_path, image_list, format="GIF", duration=1.5, loop=0)  # FPS ajustado
    
        # Mostrar el GIF en Streamlit
        st.image(temp_gif_path, caption="Evoluci√≥n de la Red de Colaboraci√≥n", use_column_width=True)

        # Permitir la descarga del GIF
        with open(temp_gif_path, "rb") as file:
            gif_bytes = file.read()
        st.download_button(
            label="üì• Descargar GIF",
            data=gif_bytes,
            file_name="Evolucion_Red_Colaboracion.gif",
            mime="image/gif"
        )
    
        os.remove(temp_gif_path)  # Eliminar el archivo despu√©s de la descarga


    # üî• Integraci√≥n con la interfaz de Streamlit
    if selected_id:
        if st.button("üé• Generar GIF de Evoluci√≥n"):
            generate_collaboration_gif(df_filtered, selected_id, id_to_name)




###################################################



elif pagina == "Equipo de trabajo":
    st.subheader("Equipo de Trabajo")

    # Informaci√≥n del equipo
    equipo = [{
               "nombre": "Dr. Santiago Arceo D√≠az",
               "foto": "ArceoS.jpg",
               "rese√±a": "Licenciado en F√≠sica, Maestro en F√≠sica y Doctor en Ciencias (Astrof√≠sica). Posdoctorante de la Universidad de Colima y profesor del Tecnol√≥gico Nacional de M√©xico Campus Colima. Cuenta con el perfil deseable, pertenece al n√∫cleo acad√©mico y es colaborador del cuerpo acad√©mico Tecnolog√≠as Emergentes y Desarrollo Web de la Maestr√≠a Sistemas Computacionales. Ha dirigido tesis de la Maestr√≠a en Sistemas Computacionales y en la Maestr√≠a en Arquitectura Sostenible y Gesti√≥n Urbana.",
               "CV": "https://scholar.google.com.mx/citations?user=3xPPTLoAAAAJ&hl=es", "contacto": "santiagoarceodiaz@gmail.com"},
           #{
           #    "nombre": "Jos√© Ram√≥n Gonz√°lez",
           #    "foto": "JR.jpeg",
           #    "rese√±a": "Estudiante de la facultad de medicina en la Universidad de Colima, cursando el servicio social en investigaci√≥n en el Centro Universitario de Investigaciones Biom√©dicas, bajo el proyecto Aplicaci√≥n de un software basado en modelos predictivos como herramienta de apoyo en el diagn√≥stico de sarcopenia en personas adultas mayores a partir de par√°metros antropom√©tricos.", "CV": "https://scholar.google.com.mx/citations?user=3xPPTLoAAAAJ&hl=es", "contacto": "jgonzalez90@ucol.mx"},
           {
               "nombre": "Dra. Xochitl Ang√©lica Ros√≠o Trujillo Trujillo",
               "foto": "DraXochilt.jpg",
               "rese√±a": "Bi√≥loga, Maestra y Doctora en Ciencias Fisiol√≥gicas con especialidad en Fisiolog√≠a. Es Profesora-Investigadora de Tiempo Completo de la Universidad de Colima. Cuenta con perfil deseable y es miembro del Sistema Nacional de Investigadores en el nivel 3. Su l√≠nea de investigaci√≥n es en Biomedicina en la que cuenta con una producci√≥n cient√≠fica de m√°s de noventa art√≠culos en revistas internacionales, varios cap√≠tulos de libro y dos libros. Imparte docencia y ha formado a m√°s de treinta estudiantes de licenciatura y de posgrado en programas acad√©micos adscritos al Sistema Nacional de Posgrado del CONAHCYT.",
               "CV": "https://portal.ucol.mx/cuib/XochitlTrujillo.htm", "contacto": "rosio@ucol.mx"},
                 {
               "nombre": "Dr. Miguel Huerta Viera",
               "foto": "DrHuerta.jpg",
               "rese√±a": "Doctor en Ciencias con especialidad en Fisiolog√≠a y Biof√≠sica. Es Profesor-Investigador Titular ‚ÄúC‚Äù del Centro Universitario de Investigaciones Biom√©dicas de la Universidad de Colima. Es miembro del Sistema Nacional de Investigadores en el nivel 3 em√©rito. Su campo de investigaci√≥n es la Biomedicina, con √©nfasis en la fisiolog√≠a y biof√≠sica del sistema neuromuscular y la fisiopatolog√≠a de la diabetes mellitus. Ha publicado m√°s de cien art√≠culos revistas indizadas al Journal of Citation Reports y ha graduado a m√°s de 40 Maestros y Doctores en Ciencias en programas SNP-CONAHCyT.",
               "CV": "https://portal.ucol.mx/cuib/dr-miguel-huerta.htm", "contacto": "huertam@ucol.mx"},
           #      {
           #    "nombre": "Dr. Jaime Alberto Bricio Barrios",
           #    "foto":  "BricioJ.jpg",
           #    "rese√±a": "Licenciado en Nutrici√≥n, Maestro en Ciencias M√©dicas, Maestro en Seguridad Alimentaria y Doctor en Ciencias M√©dicas. Profesor e Investigador de Tiempo Completo de la Facultad de Medicina en la Universidad de Colima. miembro del Sistema Nacional de Investigadores en el nivel 1. Miembro fundador de la asociaci√≥n civil DAYIN (Desarrollo de Ayuda con Investigaci√≥n)",
           #    "CV": "https://scholar.google.com.mx/citations?hl=es&user=ugl-bksAAAAJ", "contacto": "jbricio@ucol.mx"},      
               {
               "nombre": "Mtra. Elena Elsa Bricio Barrios",
               "foto": "BricioE.jpg",
               "rese√±a": "Qu√≠mica Metal√∫rgica, Maestra en Ciencias en Ingenier√≠a Qu√≠mica y doctorante en Ingenier√≠a Qu√≠mica. Actualmente es profesora del Tecnol√≥gico Nacional de M√©xico Campus Colima. Cuenta con el perfil deseable, es miembro del cuerpo acad√©mico Tecnolog√≠as Emergentes y Desarrollo Web y ha codirigido tesis de la Maestr√≠a en Sistemas Computacionales.",
               "CV": "https://scholar.google.com.mx/citations?hl=es&user=TGZGewEAAAAJ", "contacto": "elena.bricio@colima.tecnm.mx"},
          #     {
          #     "nombre": "Dra. M√≥nica R√≠os Silva",
          #     "foto": "rios.jpg",
          #     "rese√±a": "M√©dica cirujana y partera con especialidad en Medicina Interna y Doctorado en Ciencias M√©dicas por la Universidad de Colima, m√©dica especialista del Hospital Materno Infantil de Colima y PTC de la Facultad de Medicina de la Universidad de Colima. Es profesora de los posgrados en Ciencias M√©dicas, Ciencias Fisiol√≥gicas, Nutrici√≥n cl√≠nica y Ciencia ambiental global.",
          #     "CV": "https://scholar.google.com.mx/scholar?hl=en&as_sdt=0%2C5&q=Monica+Rios+silva&btnG=", "contacto": "mrios@ucol.mx"},
          #     {
          #     "nombre": "Dra. Rosa Yolitzy C√°rdenas Mar√≠a",  
          #     "foto": "cardenas.jpg",
          #     "rese√±a": "Ha realizado los estudios de Qu√≠mica Farmac√©utica Bi√≥loga, Maestr√≠a en Ciencias M√©dicas y Doctorado en Ciencias M√©dicas, todos otorgados por la Universidad de Colima. Actualmente, se desempe√±a como T√©cnica Acad√©mica Titular C en el Centro Universitario de Investigaciones Biom√©dicas de la Universidad de Colima, enfoc√°ndose en la investigaci√≥n b√°sica y cl√≠nica de enfermedades cr√≥nico-degenerativas no transmisibles en investigaci√≥n. Tambi√©n es profesora en la Maestr√≠a y Doctorado en Ciencias M√©dicas, as√≠ como en la Maestr√≠a en Nutrici√≥n Cl√≠nica de la misma universidad. Es miembro del Sistema Nacional de Investigadores nivel I y miembro fundador activo de la asociaci√≥n civil DAYIN (https://www.dayinac.org/)",
          #     "CV": "https://scholar.google.com.mx/scholar?hl=en&as_sdt=0%2C5&q=rosa+yolitzy+c%C3%A1rdenas-mar%C3%ADa&btnG=&oq=rosa+yoli", "contacto": "rosa_cardenas@ucol.mx"}
            ]

    # Establecer la altura deseada para las im√°genes
    altura_imagen = 150  # Cambia este valor seg√∫n tus preferencias

    # Mostrar informaci√≥n de cada miembro del equipo
    for miembro in equipo:
        st.subheader(miembro["nombre"])
        img = st.image(miembro["foto"], caption=f"Foto de {miembro['nombre']}", use_container_width=False, width=altura_imagen)
        st.write(f"Correo electr√≥nico: {miembro['contacto']}")
        st.write(f"Rese√±a profesional: {miembro['rese√±a']}")
        st.write(f"CV: {miembro['CV']}")

    # Informaci√≥n de contacto
    st.subheader("Informaci√≥n de Contacto")
    st.write("Si deseas ponerte en contacto con nuestro equipo, puedes enviar un correo a santiagoarceodiaz@gmail.com")

# Ruta o URL del logo
logo_path = "ucol_logo.PNG"  # Si es local, usa el nombre del archivo
#st.image("ucol_logo.PNG", width=150)  # Ajusta el ancho seg√∫n necesites
#st.image(logo_path, use_container_width=True)
    
# Crear un contenedor con tres columnas y colocar la imagen en el centro
col1, col2, col3 = st.columns([1, 2, 1])  # La columna central es m√°s ancha

with col2:  # Colocar la imagen en la columna central
    st.image(logo_path, width=400)  # Ajusta el tama√±o seg√∫n necesites

